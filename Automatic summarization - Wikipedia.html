<!DOCTYPE html>
<html class="client-js ve-not-available" dir="ltr" lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<title>Automatic summarization - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Automatic_summarization","wgTitle":"Automatic summarization","wgCurRevisionId":774545866,"wgRevisionId":774545866,"wgArticleId":637199,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Articles lacking in-text citations from March 2015","All articles lacking in-text citations","Wikipedia articles needing style editing from March 2015","All articles needing style editing","Articles to be expanded from February 2017","All articles to be expanded","Articles using small message boxes","CS1 maint: Explicit use of et al.","Computational linguistics","Natural language processing","Tasks of natural language processing","Data mining"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Automatic_summarization","wgRelevantArticleId":637199,"wgRequestId":"WOnJMQpAEKsAABPrlP0AAABS","wgIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgWikiEditorEnabledModules":{"toolbar":true,"dialogs":true,"preview":false,"publish":false},"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsShouldSendModuleToUser":false,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","usePageImages":true,"usePageDescriptions":true},"wgPreferredVariant":"en","wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesBetaFeatureEnabled":false,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikibaseItemId":"Q1394144","wgCentralAuthMobileDomain":false,"wgVisualEditorToolbarScrollOffset":0,"wgEditSubmitButtonLabelPublish":false});mw.loader.state({"ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","user":"ready","user.options":"loading","user.tokens":"loading","ext.cite.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.sectionAnchor":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready"});mw.loader.implement("user.options@0j3lz3q",function($,jQuery,require,module){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens@1dqfd7l",function ( $, jQuery, require, module ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});/*@nomin*/;

});mw.loader.load(["ext.cite.a11y","mediawiki.toc","mediawiki.action.view.postEdit","site","mediawiki.page.startup","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.legacy.wikibits","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.gadget.featured-articles-links","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"]);});</script>
<link rel="stylesheet" href="Automatic%20summarization%20-%20Wikipedia_files/load.css">
<script async="" src="Automatic%20summarization%20-%20Wikipedia_files/load_002.php"></script>
<style>
.cite-accessibility-label{ top:-99999px;clip:rect( 1px 1px 1px 1px ); clip:rect( 1px,1px,1px,1px ); position:absolute !important;padding:0 !important;border:0 !important;height:1px !important;width:1px !important; overflow:hidden}
.referencetooltip{position:absolute;list-style:none;list-style-image:none;opacity:0;font-size:10px;margin:0;z-index:5;padding:0}.referencetooltip li{border:#080086 2px solid;max-width:260px;padding:10px 8px 13px 8px;margin:0px;background-color:#F7F7F7;-webkit-box-shadow:2px 4px 2px rgba(0,0,0,0.3);-moz-box-shadow:2px 4px 2px rgba(0,0,0,0.3);box-shadow:2px 4px 2px rgba(0,0,0,0.3)}.referencetooltip li+li{margin-left:7px;margin-top:-2px;border:0;padding:0;height:3px;width:0px;background-color:transparent;-webkit-box-shadow:none;-moz-box-shadow:none;box-shadow:none;border-top:12px #080086 solid;border-right:7px transparent solid;border-left:7px transparent solid}.referencetooltip>li+li::after{content:'';border-top:8px #F7F7F7 solid;border-right:5px transparent solid;border-left:5px transparent solid;margin-top:-12px;margin-left:-5px;z-index:1;height:0px;width:0px;display:block}.client-js body .referencetooltip li li{border:none;-webkit-box-shadow:none;-moz-box-shadow:none;box-shadow:none;height:auto;width:auto;margin:auto;padding:0;position:static}.RTflipped{padding-top:13px}.referencetooltip.RTflipped li+li{position:absolute;top:2px;border-top:0;border-bottom:12px #080086 solid}.referencetooltip.RTflipped li+li::after{border-top:0;border-bottom:8px #F7F7F7 solid;position:absolute;margin-top:7px}.RTsettings{float:right;height:24px;width:24px;cursor:pointer;background-image:url(//upload.wikimedia.org/wikipedia/commons/thumb/0/05/OOjs_UI_icon_advanced.svg/24px-OOjs_UI_icon_advanced.svg.png);background-image:linear-gradient(transparent,transparent),url(//upload.wikimedia.org/wikipedia/commons/0/05/OOjs_UI_icon_advanced.svg);margin-top:-9px;margin-right:-7px;-webkit-transition:opacity 0.15s;-moz-transition:opacity 0.15s;-ms-transition:opacity 0.15s;-o-transition:opacity 0.15s;transition:opacity 0.15s;opacity:0.6;filter:alpha(opacity=60)}.RTsettings:hover{opacity:1;filter:alpha(opacity=100)}.RTTarget{border:#080086 2px solid}
.skin-vector li.GA,.skin-monobook li.GA,.skin-modern li.GA{list-style-image:url(//upload.wikimedia.org/wikipedia/commons/4/42/Monobook-bullet-ga.png)} .skin-vector li.FA,.skin-monobook li.FA{list-style-image:url(//upload.wikimedia.org/wikipedia/commons/d/d4/Monobook-bullet-star.png)}.skin-modern li.FA{list-style-image:url(//upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Modern-bullet-star.svg/9px-Modern-bullet-star.svg.png)}
.wp-teahouse-question-form{position:absolute;margin-left:auto;margin-right:auto;background-color:#f4f3f0;border:1px solid #a7d7f9;padding:1em}#wp-th-question-ask{float:right}.wp-teahouse-ask a.external{background-image:none !important}.wp-teahouse-respond-form{position:absolute;margin-left:auto;margin-right:auto;background-color:#f4f3f0;border:1px solid #a7d7f9;padding:1em}.wp-th-respond{float:right}.wp-teahouse-respond a.external{background-image:none !important}
.mw-collapsible-toggle{float:right;-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none}  .mw-content-ltr .mw-collapsible-toggle,.mw-content-rtl .mw-content-ltr .mw-collapsible-toggle{float:right} .mw-content-rtl .mw-collapsible-toggle,.mw-content-ltr .mw-content-rtl .mw-collapsible-toggle{float:left}.mw-customtoggle,.mw-collapsible-toggle{cursor:pointer} caption .mw-collapsible-toggle,.mw-content-ltr caption .mw-collapsible-toggle,.mw-content-rtl caption .mw-collapsible-toggle,.mw-content-rtl .mw-content-ltr caption .mw-collapsible-toggle,.mw-content-ltr .mw-content-rtl caption .mw-collapsible-toggle{float:none} li .mw-collapsible-toggle,.mw-content-ltr li .mw-collapsible-toggle,.mw-content-rtl li .mw-collapsible-toggle,.mw-content-rtl .mw-content-ltr li .mw-collapsible-toggle,.mw-content-ltr .mw-content-rtl li .mw-collapsible-toggle{float:none} .mw-collapsible-toggle-li{list-style:none}
.suggestions{overflow:hidden;position:absolute;top:0;left:0;width:0;border:0;z-index:1099;padding:0;margin:-1px 0 0 0}.suggestions-special{position:relative;background-color:#fff;cursor:pointer;border:solid 1px #aaa;margin:0;margin-top:-2px;display:none;padding:0.25em 0.25em;line-height:1.25em}.suggestions-results{background-color:#fff;cursor:pointer;border:solid 1px #aaa;padding:0;margin:0}.suggestions-result{color:#000;margin:0;line-height:1.5em;padding:0.01em 0.25em;text-align:left; overflow:hidden;-o-text-overflow:ellipsis; text-overflow:ellipsis;white-space:nowrap}.suggestions-result-current{background-color:#4c59a6;color:#fff}.suggestions-special .special-label{color:#808080;text-align:left}.suggestions-special .special-query{color:#000;font-style:italic;text-align:left}.suggestions-special .special-hover{background-color:#c0c0c0}.suggestions-result-current .special-label,.suggestions-result-current .special-query{color:#fff}.highlight{font-weight:bold}
@media screen {
	.tochidden,.toctoggle{-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none}.toctoggle{font-size:94%}}
@media print {
	#toc.tochidden,.toctoggle{display:none}}
.uls-menu{border-radius:4px; font-size:medium}.uls-search,.uls-language-settings-close-block{border-top-right-radius:4px;border-top-left-radius:4px}.uls-language-list{border-bottom-right-radius:4px;border-bottom-left-radius:4px}.uls-menu.callout .caret-before,.uls-menu.callout .caret-after{border-top:10px solid transparent;border-right:10px solid #c9c9c9;border-bottom:10px solid transparent;display:inline-block;left:-11px; top:17px;position:absolute}.uls-menu.callout .caret-after{border-right:10px solid #fcfcfc;display:inline-block;left:-10px}.uls-menu.callout--languageselection .caret-after{border-right:10px solid #fff}.uls-ui-languages button{margin:5px 15px 5px 0;white-space:nowrap;overflow:hidden}.uls-search-wrapper-wrapper{position:relative;padding-left:40px;margin-top:5px;margin-bottom:5px}.uls-icon-back{background:transparent url(/w/extensions/UniversalLanguageSelector/resources/images/back-grey-ltr.png?90e9b) no-repeat scroll center center;background-image:-webkit-linear-gradient( transparent,transparent ),url(/w/extensions/UniversalLanguageSelector/resources/images/back-grey-ltr.svg?ae714);background-image:linear-gradient( transparent,transparent ),url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%20standalone%3D%22no%22%3F%3E%0A%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%2024%2024%22%20id%3D%22Layer_1%22%3E%0A%20%20%20%20%3Cpath%20d%3D%22M7%2013.1l8.9%208.9c.8-.8.8-2%200-2.8l-6.1-6.1%206-6.1c.8-.8.8-2%200-2.8L7%2013.1z%22%20id%3D%22path3%22%20fill%3D%22%23555%22%2F%3E%0A%3C%2Fsvg%3E%0A);background-size:28px;background-position:center center;height:32px;width:40px;display:block;position:absolute;left:0;border-right:1px solid #c9c9c9;opacity:0.8}.uls-icon-back:hover{opacity:1;cursor:pointer}
@media print{#centralNotice{display:none}}.cn-closeButton{display:inline-block;zoom:1;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABMAAAATCAYAAAByUDbMAAACeklEQVR4Aa1UM3jkcRBN6qS//tTFttPHqmI359VZ5dm2L1zFtr02YlRx5sX2ft/7ed7O/w1M5ufnt8NZQjCBQXhG+IYZe5zjfju7xcHc3NzEzMwM6xOEqNnZ2ZeVlZW827dv1ycmJnbGx8d3Yr5582Z9eXk5D/d4h/empqYm+K0nw3yKcF4sFmdzOJzGgIAAhb29vc7Ozk6/Atrr/fz8lCwWq6m3tzcb72G3nmzFo/NNTU354eHhIltbW72Tk5M2IyND9P79+8ZPnz7VvXnzpoHBYPS4uLhobGxsDMHBwaLa2tp82MF+PVmUUqnMioiI6LOysjLQLCcPS2ZmZn7Ozc39oPtFYG80GgWpqakSS0tLY1hYmEgikWTBfoXsLD16BT3gUWhoqEKtVheREZ+Qu0IEjI+PZ2k0GqFer+cnJSWJra2tDWw2u2FqauoVeEAW3NzcnBcYGCh3dHTUtba2ltNjLvRJSEiQEQEPRENDQzkMBqPXwsKiXyqVFsFDNzc3LWmoqKmp4YIHZIyHDx9WOzg46GJiYmTk5e/h4eHstLQ0CX2ykXST4JPJ8y7sSVM5/QGMf9y4caMHf3r//v1a8IDsGRm04/D58+dtpNFPPNRqtflXrlzpI8G15IEGc2xsrFSlUglwD+Tk5NRBmuTk5A7wgOxbXFxcF8iysrIa1mskEolKvL29Ne7u7mpXV1dNaWlp9fr7X79+VYMMeQieY/dsv5p1r2g2Nja2vWZ7RXNiYuIA0dwlzwYGBjbkGXmURXeLeUa1ul2ebV8BEH+7CiAiASTYvgJ2qc3MzMzF2vz8+XPd27dvG5hMZg+CsV1tHmfXOP5+dqyddgHOI7v1srTdcwAAAABJRU5ErkJggg==);background:url(/w/extensions/CentralNotice/resources/subscribing/CloseWindow19x19.png?7596b)!ie;width:19px;height:19px;text-indent:19px;white-space:nowrap;overflow:hidden}
.mw-ui-button{font-family:inherit;font-size:1em;display:inline-block;min-width:4em;max-width:28.75em;padding:0.546875em 1em;line-height:1.286;margin:0;border-radius:2px;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;-webkit-appearance:none;*display:inline; zoom:1;vertical-align:middle;background-color:#f8f9fa;color:#222222;border:1px solid #a2a9b1;text-align:center;font-weight:bold;cursor:pointer}.mw-ui-button:visited{color:#222222}.mw-ui-button:hover{background-color:#ffffff;color:#444444;border-color:#a2a9b1}.mw-ui-button:focus{background-color:#ffffff;color:#222222;border-color:#3366cc;box-shadow:inset 0 0 0 1px #3366cc,inset 0 0 0 2px #ffffff}.mw-ui-button:active,.mw-ui-button.is-on,.mw-ui-button.mw-ui-checked{background-color:#d9d9d9;color:#000000;border-color:#72777d;box-shadow:none}.mw-ui-button:disabled{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1}.mw-ui-button:disabled:hover,.mw-ui-button:disabled:active{background-color:#c8ccd1;color:#fff;box-shadow:none;border-color:#c8ccd1}.mw-ui-button:focus{outline-width:0}.mw-ui-button:focus::-moz-focus-inner{border-color:transparent;padding:0}.mw-ui-button:not( :disabled ){-webkit-transition:background-color 100ms,color 100ms,border-color 100ms,box-shadow 100ms;-moz-transition:background-color 100ms,color 100ms,border-color 100ms,box-shadow 100ms;transition:background-color 100ms,color 100ms,border-color 100ms,box-shadow 100ms}.mw-ui-button:disabled{text-shadow:none;cursor:default}.mw-ui-button.mw-ui-big{font-size:1.3em}.mw-ui-button.mw-ui-block{display:block;width:100%;margin-left:auto;margin-right:auto}.mw-ui-button.mw-ui-progressive,.mw-ui-button.mw-ui-constructive{background-color:#3366cc;color:#fff;border:1px solid #3366cc;text-shadow:0 1px rgba(0,0,0,0.1)}.mw-ui-button.mw-ui-progressive:hover,.mw-ui-button.mw-ui-constructive:hover{background-color:#447ff5;border-color:#447ff5}.mw-ui-button.mw-ui-progressive:focus,.mw-ui-button.mw-ui-constructive:focus{box-shadow:inset 0 0 0 1px #3366cc,inset 0 0 0 2px #ffffff}.mw-ui-button.mw-ui-progressive:active,.mw-ui-button.mw-ui-constructive:active,.mw-ui-button.mw-ui-progressive.is-on,.mw-ui-button.mw-ui-constructive.is-on,.mw-ui-button.mw-ui-progressive.mw-ui-checked,.mw-ui-button.mw-ui-constructive.mw-ui-checked{background-color:#2a4b8d;border-color:#2a4b8d;box-shadow:none}.mw-ui-button.mw-ui-progressive:disabled,.mw-ui-button.mw-ui-constructive:disabled{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1}.mw-ui-button.mw-ui-progressive:disabled:hover,.mw-ui-button.mw-ui-constructive:disabled:hover,.mw-ui-button.mw-ui-progressive:disabled:active,.mw-ui-button.mw-ui-constructive:disabled:active,.mw-ui-button.mw-ui-progressive:disabled.mw-ui-checked,.mw-ui-button.mw-ui-constructive:disabled.mw-ui-checked{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1;box-shadow:none}.mw-ui-button.mw-ui-progressive.mw-ui-quiet,.mw-ui-button.mw-ui-constructive.mw-ui-quiet{color:#222222}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:hover,.mw-ui-button.mw-ui-constructive.mw-ui-quiet:hover{background-color:transparent;color:#447ff5}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:active,.mw-ui-button.mw-ui-constructive.mw-ui-quiet:active,.mw-ui-button.mw-ui-progressive.mw-ui-quiet.mw-ui-checked,.mw-ui-button.mw-ui-constructive.mw-ui-quiet.mw-ui-checked{color:#2a4b8d}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:focus,.mw-ui-button.mw-ui-constructive.mw-ui-quiet:focus{background-color:transparent;color:#3366cc}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:disabled,.mw-ui-button.mw-ui-constructive.mw-ui-quiet:disabled{color:#c8ccd1}.mw-ui-button.mw-ui-destructive{background-color:#dd3333;color:#fff;border:1px solid #dd3333;text-shadow:0 1px rgba(0,0,0,0.1)}.mw-ui-button.mw-ui-destructive:hover{background-color:#ff4242;border-color:#ff4242}.mw-ui-button.mw-ui-destructive:focus{box-shadow:inset 0 0 0 1px #dd3333,inset 0 0 0 2px #ffffff}.mw-ui-button.mw-ui-destructive:active,.mw-ui-button.mw-ui-destructive.is-on,.mw-ui-button.mw-ui-destructive.mw-ui-checked{background-color:#b32424;border-color:#b32424;box-shadow:none}.mw-ui-button.mw-ui-destructive:disabled{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1}.mw-ui-button.mw-ui-destructive:disabled:hover,.mw-ui-button.mw-ui-destructive:disabled:active,.mw-ui-button.mw-ui-destructive:disabled.mw-ui-checked{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1;box-shadow:none}.mw-ui-button.mw-ui-destructive.mw-ui-quiet{color:#222222}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:hover{background-color:transparent;color:#ff4242}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:active,.mw-ui-button.mw-ui-destructive.mw-ui-quiet.mw-ui-checked{color:#b32424}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:focus{background-color:transparent;color:#dd3333}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:disabled{color:#c8ccd1}.mw-ui-button.mw-ui-quiet{background:transparent;border:0;text-shadow:none;color:#222222}.mw-ui-button.mw-ui-quiet:hover{background-color:transparent;color:#444444}.mw-ui-button.mw-ui-quiet:active,.mw-ui-button.mw-ui-quiet.mw-ui-checked{color:#000000}.mw-ui-button.mw-ui-quiet:focus{background-color:transparent;color:#222222}.mw-ui-button.mw-ui-quiet:disabled{color:#c8ccd1}.mw-ui-button.mw-ui-quiet:hover,.mw-ui-button.mw-ui-quiet:focus{box-shadow:none}.mw-ui-button.mw-ui-quiet:active,.mw-ui-button.mw-ui-quiet:disabled{background:transparent}input.mw-ui-button::-moz-focus-inner,button.mw-ui-button::-moz-focus-inner{margin-top:-1px;margin-bottom:-1px}a.mw-ui-button{text-decoration:none}a.mw-ui-button:hover,a.mw-ui-button:focus{text-decoration:none}.mw-ui-button-group > *{min-width:48px;border-radius:0;float:left}.mw-ui-button-group > *:first-child{border-top-left-radius:2px;border-bottom-left-radius:2px}.mw-ui-button-group > *:not( :first-child ){border-left:0}.mw-ui-button-group > *:last-child{border-top-right-radius:2px;border-bottom-right-radius:2px}.mw-ui-button-group .is-on .button{cursor:default}
.mw-ui-icon{position:relative;line-height:1.5em;min-height:1.5em;min-width:1.5em}.mw-ui-icon.mw-ui-icon-element{text-indent:-999px;overflow:hidden;width:3.5em;min-width:3.5em;max-width:3.5em}.mw-ui-icon.mw-ui-icon-element:before{left:0;right:0;position:absolute;margin:0 1em}.mw-ui-icon.mw-ui-icon-before:before,.mw-ui-icon.mw-ui-icon-element:before{background-position:50% 50%;background-repeat:no-repeat;background-size:100% auto;float:left;display:block;min-height:1.5em;content:''}.mw-ui-icon.mw-ui-icon-before:before{position:relative;width:1.5em;margin-right:1em}.mw-ui-icon.mw-ui-icon-small:before{background-size:66.67% auto}
.postedit-container{margin:0 auto;position:fixed;top:0;height:0;left:50%;z-index:1000;font-size:13px}.postedit-container:hover{cursor:pointer}.postedit{position:relative;top:0.6em;left:-50%;padding:0.6em 3.6em 0.6em 1.1em;line-height:1.5625em;color:#626465;background-color:#f4f4f4;border:1px solid #dcd9d9;text-shadow:0 0.0625em 0 rgba( 255,255,255,0.5 );border-radius:5px;box-shadow:0 2px 5px 0 #ccc;-webkit-transition:all 0.25s ease-in-out;-moz-transition:all 0.25s ease-in-out;-ms-transition:all 0.25s ease-in-out;-o-transition:all 0.25s ease-in-out;transition:all 0.25s ease-in-out}.skin-monobook .postedit{top:6em !important; }.postedit-faded{opacity:0}.postedit-icon{padding-left:41px;  line-height:25px;background-repeat:no-repeat;background-position:8px 50%}.postedit-icon-checkmark{background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAB9ElEQVR4AZWRA3AYURQArxrVHtW2bdu2bdu2zdi2bdu2bWxs7zeehZaw4f70kbs+zI3e/nWK+RWx3aOFlrL56Sy5SxrruG69hlv6OyK+mz+8KDSXdXembj0ispT7tjs4ZTIbpYBvxGSGKzZTeFrb7W/meN002swFs0U8ttpHTkF2BvCqWQrW35929bTsKm5Zb+SEwWwcY8wAngB9m7Z+d+rIPZ/npdy12M5p47n8dXsCYAf0qPy06eGMdktuDu9Qf+JmKl3SWM91qzVcN9tAbEYkwMaq0tyb1m/To5kP170el/BK8/qa6sJr70ydf+T/Uu5ab+Oo/lS0AkUBpIFWlZ9WPhxpse/PHO7YbOOczjL0vZV2lNxPPtG73dYXM+xvm2znrOl83tidoqCwMBgYXsPFB0on5S6pr+eK5TKuW67lgvaKvF8mL1dtfTL32FHxRdyx3cQpg7m4x9sCXKkTIzA4LDH44zWdzaUf71hv5rTG4uyzcusybxSX7aThbMQ8XgCYAp3rzTTQOiIh9PNlzY3FSuZxrzjme1Y7uGS6kjsWO4jPjM4FVjRZsvD4kO9XtTZzQn82NyzWc0B7AmZh6gA/hOYSGhfw9YbOVnarj+S7800AL2BIsxUAbWNToj7bhBuQmZcOsFdoKUC74rGheCwXmqAIQTc9jQcrADIAAAAASUVORK5CYII=);background-image:url(/w/resources/src/mediawiki.action/images/green-checkmark.png?d94f1)!ie;background-position:left}.postedit-close{position:absolute;padding:0 0.8em;right:0;top:0;font-size:1.25em;font-weight:bold;line-height:2.3em;color:#000;text-shadow:0 0.0625em 0 #fff;text-decoration:none;opacity:0.2;filter:alpha( opacity=20 )}.postedit-close:hover{color:#000;text-decoration:none;opacity:0.4;filter:alpha( opacity=40 )}
@-webkit-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-webkit-transform:translateY(-20px)}100%{opacity:1;-webkit-transform:translateY(0)}}@-moz-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-moz-transform:translateY(-20px)}100%{opacity:1;-moz-transform:translateY(0)}}@-o-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-o-transform:translateY(-20px)}100%{opacity:1;-o-transform:translateY(0)}}@keyframes centralAuthPPersonalAnimation{0%{opacity:0;transform:translateY(-20px)}100%{opacity:1;transform:translateY(0)}}.centralAuthPPersonalAnimation{-webkit-animation-duration:1s;-moz-animation-duration:1s;-o-animation-duration:1s;animation-duration:1s;-webkit-animation-fill-mode:both;-moz-animation-fill-mode:both;-o-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:centralAuthPPersonalAnimation;-moz-animation-name:centralAuthPPersonalAnimation;-o-animation-name:centralAuthPPersonalAnimation;animation-name:centralAuthPPersonalAnimation}
.ext-quick-survey-panel,.ext-qs-loader-bar{width:auto;background-color:#eaecf0} .ext-qs-loader-bar{display:block;height:100px;margin-left:1.4em;clear:right;float:right;background-color:#eaecf0}.ext-qs-loader-bar.mw-ajax-loader{top:0}@media all and (min-width:720px){.ext-qs-loader-bar,.ext-quick-survey-panel{margin-left:1.4em;width:300px;clear:right;float:right}}</style><style>
.suggestions a.mw-searchSuggest-link,.suggestions a.mw-searchSuggest-link:hover,.suggestions a.mw-searchSuggest-link:active,.suggestions a.mw-searchSuggest-link:focus{color:#000;text-decoration:none}.suggestions-result-current a.mw-searchSuggest-link,.suggestions-result-current a.mw-searchSuggest-link:hover,.suggestions-result-current a.mw-searchSuggest-link:active,.suggestions-result-current a.mw-searchSuggest-link:focus{color:#fff}.suggestions a.mw-searchSuggest-link .special-query{ overflow:hidden;-o-text-overflow:ellipsis; text-overflow:ellipsis;white-space:nowrap}
#uls-settings-block{border-top:1px solid #c9c9c9;background:#f8f8f8;background:-webkit-gradient( linear,left top,left bottom,from( #fbfbfb ),to( #f0f0f0 ) );background:-webkit-linear-gradient( top,#fbfbfb,#f0f0f0 );background:-moz-linear-gradient( top,#fbfbfb,#f0f0f0 );background:-o-linear-gradient( top,#fbfbfb,#f0f0f0 );background:linear-gradient( #fbfbfb,#f0f0f0 );padding-left:10px;line-height:1.2em;border-radius:0 0 5px 5px}#uls-settings-block div.display-settings-block,#uls-settings-block div.input-settings-block{display:inline-block;margin:8px 15px;color:#565656}#uls-settings-block div.display-settings-block:hover,#uls-settings-block div.input-settings-block:hover{color:#252525}
.mw-mmv-overlay{position:fixed;top:0;left:0;right:0;bottom:0;z-index:1000;background-color:#000}body.mw-mmv-lightbox-open{overflow-y:auto;  }body.mw-mmv-lightbox-open #mw-page-base,body.mw-mmv-lightbox-open #mw-head-base,body.mw-mmv-lightbox-open #mw-navigation,body.mw-mmv-lightbox-open #content,body.mw-mmv-lightbox-open #footer,body.mw-mmv-lightbox-open #globalWrapper{ display:none}body.mw-mmv-lightbox-open > *{ display:none}body.mw-mmv-lightbox-open > .mw-mmv-overlay,body.mw-mmv-lightbox-open > .mw-mmv-wrapper{display:block}.mw-mmv-filepage-buttons{margin-top:5px}.mw-mmv-filepage-buttons .mw-mmv-view-expanded,.mw-mmv-filepage-buttons .mw-mmv-view-config{display:block;line-height:inherit}.mw-mmv-filepage-buttons .mw-mmv-view-expanded.mw-ui-icon:before{background-image:url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%20standalone%3D%22no%22%3F%3E%0A%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%201024%20768%22%3E%0A%20%20%20%20%3Cg%20fill%3D%22%2372777d%22%3E%0A%20%20%20%20%20%20%20%20%3Cpath%20d%3D%22M851.2%2071.6L690.7%20232.1l-40.1-40.3-9.6%20164.8%20164.8-9.3-40.3-40.4L926%20146.4l58.5%2058.5L997.6%200%20792.7%2013.1%22%2F%3E%0A%20%20%20%20%20%20%20%20%3Cpath%20d%3D%22M769.6%2089.3H611.9l70.9%2070.8%207.9%207.5m-47.1%20234.6l-51.2%203%203-51.2%209.4-164.4%205.8-100.3H26.4V768h883.1V387l-100.9%205.8-165%209.4zM813.9%20678H113.6l207.2-270.2%2031.5-12.9L548%20599.8l105.9-63.2%20159.8%20140.8.2.6zm95.6-291.9V228l-79.1%2078.9%207.8%207.9%22%2F%3E%0A%20%20%20%20%3C%2Fg%3E%0A%3C%2Fsvg%3E%0A)}.mw-mmv-filepage-buttons .mw-mmv-view-config.mw-ui-icon:before{background-image:url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%20standalone%3D%22no%22%3F%3E%0A%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%201024%20768%22%3E%0A%20%20%20%20%3Cpath%20d%3D%22M897%20454.6V313.4L810.4%20299c-6.4-23.3-16-45.7-27.3-65.8l50.5-71.4-99.4-100.2-71.4%2050.5c-20.9-11.2-42.5-20.9-65.8-27.3L582.6-1H441.4L427%2085.6c-23.3%206.4-45.7%2016-65.8%2027.3l-71.4-50.5-100.3%2099.5%2050.5%2071.4c-11.2%2020.9-20.9%2042.5-27.3%2066.6L127%20313.4v141.2l85.8%2014.4c6.4%2023.3%2016%2045.7%2027.3%2066.6L189.6%20607l99.5%2099.5%2071.4-50.5c20.9%2011.2%2042.5%2020.9%2066.6%2027.3l14.4%2085.8h141.2l14.4-86.6c23.3-6.4%2045.7-16%2065.8-27.3l71.4%2050.5%2099.5-99.5-50.5-71.4c11.2-20.9%2020.9-42.5%2027.3-66.6l86.4-13.6zm-385%2077c-81.8%200-147.6-66.6-147.6-147.6%200-81.8%2066.6-147.6%20147.6-147.6S659.6%20302.2%20659.6%20384%20593.8%20531.6%20512%20531.6z%22%20fill%3D%22%2372777d%22%2F%3E%0A%3C%2Fsvg%3E%0A);opacity:0.75}.mw-mmv-filepage-buttons .mw-mmv-view-config.mw-ui-icon:before:hover{opacity:1}
.ve-activated #toc,.ve-activated #siteNotice,.ve-activated .mw-indicators,.ve-activated #t-print,.ve-activated #t-permalink,.ve-activated #p-coll-print_export,.ve-activated #t-cite,.ve-deactivating .ve-ui-surface,.ve-active .ve-init-mw-desktopArticleTarget-editableContent{display:none} .ve-activating .ve-ui-surface{height:0;padding:0 !important; overflow:hidden} .ve-loading #content > :not( .ve-init-mw-desktopArticleTarget-loading-overlay ), .ve-activated .ve-init-mw-desktopArticleTarget-uneditableContent{  pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none; opacity:0.5}.ve-activated #catlinks{cursor:pointer}.ve-activated #catlinks a{opacity:1} .ve-activated #content{position:relative} .ve-init-mw-desktopArticleTarget-loading-overlay{position:absolute;left:0;right:0;z-index:1;margin-top:-0.5em}.ve-init-mw-desktopArticleTarget-progress{height:1em;overflow:hidden;margin:0 25%}.ve-init-mw-desktopArticleTarget-progress-bar{height:1em;width:0} .mw-editsection{white-space:nowrap; unicode-bidi:-moz-isolate;unicode-bidi:-webkit-isolate;unicode-bidi:isolate}.mw-editsection-divider{color:#555} .ve-init-mw-desktopArticleTarget-progress{height:0.75em;border:1px solid #36c;background:#fff;border-radius:2px;box-shadow:0 0.1em 0 0 rgba( 0,0,0,0.15 )}.ve-init-mw-desktopArticleTarget-progress-bar{height:0.75em;background:#36c}</style><meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="Automatic%20summarization%20-%20Wikipedia_files/load_002.css">
<meta name="generator" content="MediaWiki 1.29.0-wmf.19">
<meta name="referrer" content="origin-when-cross-origin">
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Automatic_summarization">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit">
<link rel="edit" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit">
<link rel="apple-touch-icon" href="https://en.wikipedia.org/static/apple-touch/wikipedia.png">
<link rel="shortcut icon" href="https://en.wikipedia.org/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="https://en.wikipedia.org/w/opensearch_desc.php" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://en.wikipedia.org/w/api.php?action=rsd">
<link rel="copyright" href="https://creativecommons.org/licenses/by-sa/3.0/">
<link rel="canonical" href="https://en.wikipedia.org/wiki/Automatic_summarization">
<link rel="dns-prefetch" href="https://login.wikimedia.org/">
<link rel="dns-prefetch" href="https://meta.wikimedia.org/">
<script src="Automatic%20summarization%20-%20Wikipedia_files/load.php"></script></head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-Automatic_summarization rootpage-Automatic_summarization skin-vector action-view">		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>

							<div id="siteNotice" class="mw-body-content"><div id="centralNotice"></div><!-- CentralNotice --></div>
						<div class="mw-indicators mw-body-content">
</div>
			<h1 id="firstHeading" class="firstHeading" lang="en">Automatic summarization</h1>
									<div id="bodyContent" class="mw-body-content">
									<div id="siteSub">From Wikipedia, the free encyclopedia</div>
								<div id="contentSub"></div>
												<div id="jump-to-nav" class="mw-jump">
					Jump to:					<a href="#mw-head">navigation</a>, 					<a href="#p-search">search</a>
				</div>
				<div id="mw-content-text" dir="ltr" class="mw-content-ltr" lang="en"><table class="plainlinks metadata ambox ambox-content ambox-multiple_issues compact-ambox" role="presentation">
<tbody><tr>
<td class="mbox-image">
<div style="width:52px"><img alt="" src="Automatic%20summarization%20-%20Wikipedia_files/40px-Ambox_important.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Ambox_important.svg/60px-Ambox_important.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Ambox_important.svg/80px-Ambox_important.svg.png 2x" data-file-width="40" data-file-height="40" width="40" height="40"></div>
</td>
<td class="mbox-text">
<div class="mw-collapsible" style="width:95%; margin: 0.2em 0;"><span class="mw-collapsible-toggle"><span class="mw-collapsible-bracket">[</span><a role="button" tabindex="0">hide</a><span class="mw-collapsible-bracket">]</span></span><span class="mbox-text-span"><b>This article has multiple issues.</b> Please help <b><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit">improve it</a></b> or discuss these issues on the <b><a href="https://en.wikipedia.org/wiki/Talk:Automatic_summarization" title="Talk:Automatic summarization">talk page</a></b>. <small><i>(<a href="https://en.wikipedia.org/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove these template messages</a>)</i></small></span>
<div class="mw-collapsible-content" style="margin-top: 0.3em;">
<table class="plainlinks metadata ambox ambox-style ambox-More_footnotes" role="presentation">
<tbody><tr>
<td class="mbox-image">
<div style="width:52px"><img alt="" src="Automatic%20summarization%20-%20Wikipedia_files/40px-Text_document_with_red_question_mark.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/60px-Text_document_with_red_question_mark.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/80px-Text_document_with_red_question_mark.svg.png 2x" data-file-width="48" data-file-height="48" width="40" height="40"></div>
</td>
<td class="mbox-text"><span class="mbox-text-span">This article includes a <a href="https://en.wikipedia.org/wiki/Wikipedia:Citing_sources" title="Wikipedia:Citing sources">list of references</a>, but <b>its sources remain unclear</b> because it has <b>insufficient <a href="https://en.wikipedia.org/wiki/Wikipedia:Citing_sources#Inline_citations" title="Wikipedia:Citing sources">inline citations</a></b>. <span class="hide-when-compact">Please help to <a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Fact_and_Reference_Check" title="Wikipedia:WikiProject Fact and Reference Check">improve</a> this article by <a href="https://en.wikipedia.org/wiki/Wikipedia:When_to_cite" title="Wikipedia:When to cite">introducing</a> more precise citations.</span> <small><i>(March 2015)</i></small> <small class="hide-when-compact"><i>(<a href="https://en.wikipedia.org/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></span></td>
</tr>
</tbody></table>
<table class="plainlinks metadata ambox ambox-style ambox-Tone" role="presentation">
<tbody><tr>
<td class="mbox-image">
<div style="width:52px"><img alt="" src="Automatic%20summarization%20-%20Wikipedia_files/40px-Edit-clear.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/60px-Edit-clear.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/80px-Edit-clear.svg.png 2x" data-file-width="48" data-file-height="48" width="40" height="40"></div>
</td>
<td class="mbox-text"><span class="mbox-text-span">This article's <b>tone or style may not reflect the <a href="https://en.wikipedia.org/wiki/Wikipedia:Writing_better_articles#Tone" title="Wikipedia:Writing better articles">encyclopedic tone</a> used on Wikipedia</b>. <span class="hide-when-compact">See Wikipedia's <a href="https://en.wikipedia.org/wiki/Wikipedia:Writing_better_articles#Tone" title="Wikipedia:Writing better articles">guide to writing better articles</a> for suggestions.</span> <small><i>(March 2015)</i></small> <small class="hide-when-compact"><i>(<a href="https://en.wikipedia.org/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></span></td>
</tr>
</tbody></table>
</div>
</div>
<small class="hide-when-compact"><i>(<a href="https://en.wikipedia.org/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></td>
</tr>
</tbody></table>
<p><b>Automatic summarization</b> is the process of reducing a text document with a <a href="https://en.wikipedia.org/wiki/Computer_program" title="Computer program">computer program</a> in order to create a <a href="https://en.wikipedia.org/wiki/Abstract_%28summary%29" title="Abstract (summary)">summary</a>
 that retains the most important points of the original document. 
Technologies that can make a coherent summary take into account 
variables such as length, writing style and <a href="https://en.wikipedia.org/wiki/Syntax" title="Syntax">syntax</a>. Automatic data summarization is part of <a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine learning">machine learning</a> and <a href="https://en.wikipedia.org/wiki/Data_mining" title="Data mining">data mining</a>. The main idea of summarization is to find a representative subset of the data, which contains the <i>information</i>
 of the entire set. Summarization technologies are used in a large 
number of sectors in industry today. An example of the use of 
summarization technology is <a href="https://en.wikipedia.org/wiki/Search_engine" class="mw-redirect" title="Search engine">search engines</a> such as <a href="https://en.wikipedia.org/wiki/Google" title="Google">Google</a>.
 Other examples include document summarization, image collection 
summarization and video summarization. Document summarization, tries to 
automatically create a <i>representative summary</i> or <i>abstract</i> of the entire document, by finding the most <i>informative</i>
 sentences. Similarly, in image summarization the system finds the most 
representative and important (or salient) images. Similarly, in consumer
 videos one would want to remove the boring or repetitive scenes, and 
extract out a much shorter and concise version of the video. This is 
also important, say for surveillance videos, where one might want to 
extract only important events in the recorded video, since most part of 
the video may be uninteresting with nothing going on. As the problem of <a href="https://en.wikipedia.org/wiki/Information_overload" title="Information overload">information overload</a> grows, and as the amount of data increases, the interest in automatic summarization is also increasing.</p>
<p>Generally, there are two approaches to automatic summarization: <i><a href="https://en.wiktionary.org/wiki/Extraction" class="extiw" title="wiktionary:Extraction">extraction</a></i> and <i><a href="https://en.wikipedia.org/wiki/Abstract_%28summary%29" title="Abstract (summary)">abstraction</a></i>.
 Extractive methods work by selecting a subset of existing words, 
phrases, or sentences in the original text to form the summary. In 
contrast, abstractive methods build an internal semantic representation 
and then use natural language generation techniques to create a summary 
that is closer to what a human might generate. Such a summary might 
contain words not explicitly present in the original. Research into 
abstractive methods is an increasingly important and active research 
area, however due to complexity constraints, research to date has 
focused primarily on extractive methods. In some application domains, 
extractive summarization makes more sense. Examples of these include 
image collection summarization and video summarization.</p>
<p></p>
<div id="toc" class="toc">
<div id="toctitle">
<h2>Contents</h2>
<span class="toctoggle">&nbsp;[<a role="button" tabindex="0" id="togglelink">hide</a>]&nbsp;</span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Types"><span class="tocnumber">1</span> <span class="toctext">Types</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Extraction-based_summarization"><span class="tocnumber">1.1</span> <span class="toctext">Extraction-based summarization</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Abstraction-based_summarization"><span class="tocnumber">1.2</span> <span class="toctext">Abstraction-based summarization</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Aided_summarization"><span class="tocnumber">1.3</span> <span class="toctext">Aided summarization</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-5"><a href="#Applications_and_systems_for_summarization"><span class="tocnumber">2</span> <span class="toctext">Applications and systems for summarization</span></a>
<ul>
<li class="toclevel-2 tocsection-6"><a href="#Keyphrase_extraction"><span class="tocnumber">2.1</span> <span class="toctext">Keyphrase extraction</span></a>
<ul>
<li class="toclevel-3 tocsection-7"><a href="#Supervised_learning_approaches"><span class="tocnumber">2.1.1</span> <span class="toctext">Supervised learning approaches</span></a></li>
<li class="toclevel-3 tocsection-8"><a href="#Unsupervised_approach:_TextRank"><span class="tocnumber">2.1.2</span> <span class="toctext">Unsupervised approach: TextRank</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-9"><a href="#Document_summarization"><span class="tocnumber">2.2</span> <span class="toctext">Document summarization</span></a>
<ul>
<li class="toclevel-3 tocsection-10"><a href="#Supervised_learning_approaches_2"><span class="tocnumber">2.2.1</span> <span class="toctext">Supervised learning approaches</span></a></li>
<li class="toclevel-3 tocsection-11"><a href="#Maximum_entropy-based_summarization"><span class="tocnumber">2.2.2</span> <span class="toctext">Maximum entropy-based summarization</span></a></li>
<li class="toclevel-3 tocsection-12"><a href="#TextRank_and_LexRank"><span class="tocnumber">2.2.3</span> <span class="toctext">TextRank and LexRank</span></a></li>
<li class="toclevel-3 tocsection-13"><a href="#Multi-document_summarization"><span class="tocnumber">2.2.4</span> <span class="toctext">Multi-document summarization</span></a>
<ul>
<li class="toclevel-4 tocsection-14"><a href="#Incorporating_diversity"><span class="tocnumber">2.2.4.1</span> <span class="toctext">Incorporating diversity</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-2 tocsection-15"><a href="#Submodular_Functions_as_generic_tools_for_summarization"><span class="tocnumber">2.3</span> <span class="toctext">Submodular Functions as generic tools for summarization</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="#Applications"><span class="tocnumber">2.4</span> <span class="toctext">Applications</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-17"><a href="#Evaluation_techniques"><span class="tocnumber">3</span> <span class="toctext">Evaluation techniques</span></a>
<ul>
<li class="toclevel-2 tocsection-18"><a href="#Intrinsic_and_extrinsic_evaluation"><span class="tocnumber">3.1</span> <span class="toctext">Intrinsic and extrinsic evaluation</span></a></li>
<li class="toclevel-2 tocsection-19"><a href="#Inter-textual_and_intra-textual"><span class="tocnumber">3.2</span> <span class="toctext">Inter-textual and intra-textual</span></a></li>
<li class="toclevel-2 tocsection-20"><a href="#Domain_specific_versus_domain_independent_summarization_techniques"><span class="tocnumber">3.3</span> <span class="toctext">Domain specific versus domain independent summarization techniques</span></a></li>
<li class="toclevel-2 tocsection-21"><a href="#Evaluating_summaries_qualitatively"><span class="tocnumber">3.4</span> <span class="toctext">Evaluating summaries qualitatively</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-22"><a href="#See_also"><span class="tocnumber">4</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-23"><a href="#References"><span class="tocnumber">5</span> <span class="toctext">References</span></a>
<ul>
<li class="toclevel-2 tocsection-24"><a href="#Further_reading"><span class="tocnumber">5.1</span> <span class="toctext">Further reading</span></a></li>
</ul>
</li>
</ul>
</div>
<p></p>
<h2><span class="mw-headline" id="Types">Types</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=1" title="Edit section: Types">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Extraction-based_summarization">Extraction-based summarization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=2" title="Edit section: Extraction-based summarization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In this summarization task, the automatic system extracts objects 
from the entire collection, without modifying the objects themselves. 
Examples of this include keyphrase extraction, where the goal is to 
select individual words or phrases to "tag" a document, and document 
summarization, where the goal is to select whole sentences (without 
modifying them) to create a short paragraph summary. Similarly, in image
 collection summarization, the system extracts images from the 
collection without modifying the images themselves.</p>
<h3><span class="mw-headline" id="Abstraction-based_summarization">Abstraction-based summarization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=3" title="Edit section: Abstraction-based summarization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Extraction techniques merely copy the information deemed most 
important by the system to the summary (for example, key clauses, 
sentences or paragraphs), while abstraction involves paraphrasing 
sections of the <a href="https://en.wikipedia.org/wiki/Source_document" title="Source document">source document</a>.
 In general, abstraction can condense a text more strongly than 
extraction, but the programs that can do this are harder to develop as 
they require use of <a href="https://en.wikipedia.org/wiki/Natural_language_generation" title="Natural language generation">natural language generation</a> technology, which itself is a growing field.</p>
<p>While some work has been done in abstractive summarization (creating 
an abstract synopsis like that of a human), the majority of 
summarization systems are extractive (selecting a subset of sentences to
 place in a summary).</p>
<h3><span class="mw-headline" id="Aided_summarization">Aided summarization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=4" title="Edit section: Aided summarization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine learning">Machine learning</a> techniques from closely related fields such as <a href="https://en.wikipedia.org/wiki/Information_retrieval" title="Information retrieval">information retrieval</a> or <a href="https://en.wikipedia.org/wiki/Text_mining" title="Text mining">text mining</a> have been successfully adapted to help automatic summarization.</p>
<p>Apart from Fully Automated Summarizers (FAS), there are systems that 
aid users with the task of summarization (MAHS = Machine Aided Human 
Summarization), for example by highlighting candidate passages to be 
included in the summary, and there are systems that depend on 
post-processing by a human (HAMS = Human Aided Machine Summarization).</p>
<h2><span class="mw-headline" id="Applications_and_systems_for_summarization">Applications and systems for summarization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=5" title="Edit section: Applications and systems for summarization">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>There are broadly two types of extractive summarization tasks 
depending on what the summarization program focuses on. The first is <i>generic summarization</i>,
 which focuses on obtaining a generic summary or abstract of the 
collection (whether documents, or sets of images, or videos, news 
stories etc.). The second is <i>query relevant summarization</i>, sometimes called <i>query-based summarization</i>,
 which summarizes objects specific to a query. Summarization systems are
 able to create both query relevant text summaries and generic 
machine-generated summaries depending on what the user needs.</p>
<p>An example of a summarization problem is document summarization, 
which attempts to automatically produce an abstract from a given 
document. Sometimes one might be interested in generating a summary from
 a single source document, while others can use multiple source 
documents (for example, a <a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">cluster</a> of articles on the same topic). This problem is called <a href="https://en.wikipedia.org/wiki/Multi-document_summarization" title="Multi-document summarization">multi-document summarization</a>.
 A related application is summarizing news articles. Imagine a system, 
which automatically pulls together news articles on a given topic (from 
the web), and concisely represents the latest news as a summary.</p>
<p>Image collection summarization is another application example of 
automatic summarization. It consists in selecting a representative set 
of images from a larger set of images.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">[1]</a></sup> A summary in this context is useful to show the most representative images of results in an <a href="https://en.wikipedia.org/wiki/Image_collection_exploration" title="Image collection exploration">image collection exploration</a>
 system. Video summarization is a related domain, where the system 
automatically creates a trailer of a long video. This also has 
applications in consumer or personal videos, where one might want to 
skip the boring or repetitive actions. Similarly, in surveillance 
videos, one would want to extract important and suspicious activity, 
while ignoring all the boring and redundant frames captured.</p>
<p>At a very high level, summarization algorithms try to find subsets of
 objects (like set of sentences, or a set of images), which cover 
information of the entire set. This is also called the <i>core-set</i>. 
These algorithms model notions like diversity, coverage, information and
 representativeness of the summary. Query based summarization 
techniques, additionally model for relevance of the summary with the 
query. Some techniques and algorithms which naturally model 
summarization problems are TextRank and PageRank, <a href="https://en.wikipedia.org/wiki/Submodular_set_function" title="Submodular set function">Submodular set function</a>, <a href="https://en.wikipedia.org/wiki/Determinantal_point_process" title="Determinantal point process">Determinantal point process</a>, maximal marginal relevance (MMR) etc.</p>
<h3><span class="mw-headline" id="Keyphrase_extraction">Keyphrase extraction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=6" title="Edit section: Keyphrase extraction">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The task is the following. You are given a piece of text, such as a 
journal article, and you must produce a list of keywords or key[phrase]s
 that capture the primary topics discussed in the text. In the case of <a href="https://en.wikipedia.org/wiki/Research_article" class="mw-redirect" title="Research article">research articles</a>,
 many authors provide manually assigned keywords, but most text lacks 
pre-existing keyphrases. For example, news articles rarely have 
keyphrases attached, but it would be useful to be able to automatically 
do so for a number of applications discussed below. Consider the example
 text from a news article:</p>
<dl>
<dd>"The Army Corps of Engineers, rushing to meet President Bush's 
promise to protect New Orleans by the start of the 2006 hurricane 
season, installed defective flood-control pumps last year despite 
warnings from its own expert that the equipment would fail during a 
storm, according to documents obtained by The Associated Press".</dd>
</dl>
<p>A keyphrase extractor might select "Army Corps of Engineers", 
"President Bush", "New Orleans", and "defective flood-control pumps" as 
keyphrases. These are pulled directly from the text. In contrast, an 
abstractive keyphrase system would somehow internalize the content and 
generate keyphrases that do not appear in the text, but more closely 
resemble what a human might produce, such as "political negligence" or 
"inadequate protection from floods". Abstraction requires a deep <a href="https://en.wikipedia.org/wiki/Natural_language_understanding" title="Natural language understanding">understanding of the text</a>,
 which makes it difficult for a computer system. Keyphrases have many 
applications. They can enable document browsing by providing a short 
summary, improve <a href="https://en.wikipedia.org/wiki/Information_retrieval" title="Information retrieval">information retrieval</a> (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a <a href="https://en.wikipedia.org/wiki/Full-text_search" title="Full-text search">full-text search</a>), and be employed in generating index entries for a large text corpus.</p>
<p>Depending on the different literature and the definition of key terms, words or phrases, highly related theme is certainly the <a href="https://en.wikipedia.org/wiki/Keyword_extraction" title="Keyword extraction">Keyword extraction</a>.</p>
<h4><span class="mw-headline" id="Supervised_learning_approaches">Supervised learning approaches</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=7" title="Edit section: Supervised learning approaches">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Beginning with the work of Turney,<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">[2]</a></sup> many researchers have approached keyphrase extraction as a <a href="https://en.wikipedia.org/wiki/Supervised_machine_learning" class="mw-redirect" title="Supervised machine learning">supervised machine learning</a> problem. Given a document, we construct an example for each <a href="https://en.wikipedia.org/wiki/Unigram" class="mw-redirect" title="Unigram">unigram</a>, <a href="https://en.wikipedia.org/wiki/Bigram" title="Bigram">bigram</a>,
 and trigram found in the text (though other text units are also 
possible, as discussed below). We then compute various features 
describing each example (e.g., does the phrase begin with an upper-case 
letter?). We assume there are known keyphrases available for a set of 
training documents. Using the known keyphrases, we can assign positive 
or negative labels to the examples. Then we learn a classifier that can 
discriminate between positive and negative examples as a function of the
 features. Some classifiers make a <a href="https://en.wikipedia.org/wiki/Binary_classification" title="Binary classification">binary classification</a>
 for a test example, while others assign a probability of being a 
keyphrase. For instance, in the above text, we might learn a rule that 
says phrases with initial capital letters are likely to be keyphrases. 
After training a learner, we can select keyphrases for test documents in
 the following manner. We apply the same example-generation strategy to 
the test documents, then run each example through the learner. We can 
determine the keyphrases by looking at binary classification decisions 
or probabilities returned from our learned model. If probabilities are 
given, a threshold is used to select the keyphrases. Keyphrase 
extractors are generally evaluated using precision and recall. Precision
 measures how many of the proposed keyphrases are actually correct. 
Recall measures how many of the true keyphrases your system proposed. 
The two measures can be combined in an F-score, which is the harmonic 
mean of the two (<i>F</i>&nbsp;=&nbsp;2<i>PR</i>/(<i>P</i>&nbsp;+&nbsp;<i>R</i>)
 ). Matches between the proposed keyphrases and the known keyphrases can
 be checked after stemming or applying some other text normalization.</p>
<p>Designing a supervised keyphrase extraction system involves deciding 
on several choices (some of these apply to unsupervised, too). The first
 choice is exactly how to generate examples. Turney and others have used
 all possible unigrams, bigrams, and trigrams without intervening 
punctuation and after removing stopwords. Hulth showed that you can get 
some improvement by selecting examples to be sequences of tokens that 
match certain patterns of part-of-speech tags. Ideally, the mechanism 
for generating examples produces all the known labeled keyphrases as 
candidates, though this is often not the case. For example, if we use 
only unigrams, bigrams, and trigrams, then we will never be able to 
extract a known keyphrase containing four words. Thus, recall may 
suffer. However, generating too many examples can also lead to low 
precision.</p>
<p>We also need to create features that describe the examples and are 
informative enough to allow a learning algorithm to discriminate 
keyphrases from non- keyphrases. Typically features involve various term
 frequencies (how many times a phrase appears in the current text or in a
 larger corpus), the length of the example, relative position of the 
first occurrence, various boolean syntactic features (e.g., contains all
 caps), etc. The Turney paper used about 12 such features. Hulth uses a 
reduced set of features, which were found most successful in the KEA 
(Keyphrase Extraction Algorithm) work derived from Turneys seminal 
paper.</p>
<p>In the end, the system will need to return a list of keyphrases for a
 test document, so we need to have a way to limit the number. Ensemble 
methods (i.e., using votes from several classifiers) have been used to 
produce numeric scores that can be thresholded to provide a 
user-provided number of keyphrases. This is the technique used by Turney
 with C4.5 decision trees. Hulth used a single binary classifier so the 
learning algorithm implicitly determines the appropriate number.</p>
<p>Once examples and features are created, we need a way to learn to 
predict keyphrases. Virtually any supervised learning algorithm could be
 used, such as decision trees, <a href="https://en.wikipedia.org/wiki/Naive_Bayes" class="mw-redirect" title="Naive Bayes">Naive Bayes</a>,
 and rule induction. In the case of Turney's GenEx algorithm, a genetic 
algorithm is used to learn parameters for a domain-specific keyphrase 
extraction algorithm. The extractor follows a series of heuristics to 
identify keyphrases. The genetic algorithm optimizes parameters for 
these heuristics with respect to performance on training documents with 
known key phrases.</p>
<h4><span class="mw-headline" id="Unsupervised_approach:_TextRank">Unsupervised approach: TextRank</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=8" title="Edit section: Unsupervised approach: TextRank">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Another keyphrase extraction algorithm is TextRank. While supervised 
methods have some nice properties, like being able to produce 
interpretable rules for what features characterize a keyphrase, they 
also require a large amount of <a href="https://en.wikipedia.org/wiki/Training_set" class="mw-redirect" title="Training set">training data</a>.
 Many documents with known keyphrases are needed. Furthermore, training 
on a specific domain tends to customize the extraction process to that 
domain, so the resulting classifier is not necessarily portable, as some
 of Turney's results demonstrate. Unsupervised keyphrase extraction 
removes the need for training data. It approaches the problem from a 
different angle. Instead of trying to learn explicit features that 
characterize keyphrases, the TextRank algorithm<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">[3]</a></sup> exploits the structure of the text itself to determine keyphrases that appear "central" to the text in the same way that <a href="https://en.wikipedia.org/wiki/PageRank" title="PageRank">PageRank</a> selects important Web pages. Recall this is based on the notion of "prestige" or "recommendation" from <a href="https://en.wikipedia.org/wiki/Social_network" title="Social network">social networks</a>.
 In this way, TextRank does not rely on any previous training data at 
all, but rather can be run on any arbitrary piece of text, and it can 
produce output simply based on the text's intrinsic properties. Thus the
 algorithm is easily portable to new domains and languages.</p>
<p>TextRank is a general purpose <a href="https://en.wikipedia.org/wiki/Graph_%28abstract_data_type%29" title="Graph (abstract data type)">graph</a>-based ranking algorithm for <a href="https://en.wikipedia.org/wiki/Natural_language_processing" title="Natural language processing">NLP</a>.
 Essentially, it runs PageRank on a graph specially designed for a 
particular NLP task. For keyphrase extraction, it builds a graph using 
some set of text units as vertices. Edges are based on some measure of 
semantic or <a href="https://en.wikipedia.org/wiki/Lexical_%28semiotics%29" class="mw-redirect" title="Lexical (semiotics)">lexical</a> <a href="https://en.wikipedia.org/wiki/Semantic_similarity" title="Semantic similarity">similarity</a>
 between the text unit vertices. Unlike PageRank, the edges are 
typically undirected and can be weighted to reflect a degree of 
similarity. Once the graph is constructed, it is used to form a 
stochastic matrix, combined with a damping factor (as in the "random 
surfer model"), and the ranking over vertices is obtained by finding the
 eigenvector corresponding to <a href="https://en.wikipedia.org/wiki/Eigenvalue" class="mw-redirect" title="Eigenvalue">eigenvalue</a> 1 (i.e., the <a href="https://en.wikipedia.org/wiki/Stationary_distribution" title="Stationary distribution">stationary distribution</a> of the <a href="https://en.wikipedia.org/wiki/Random_walk" title="Random walk">random walk</a> on the graph).</p>
<p>The vertices should correspond to what we want to rank. Potentially, 
we could do something similar to the supervised methods and create a 
vertex for each unigram, bigram, trigram, etc. However, to keep the 
graph small, the authors decide to rank individual unigrams in a first 
step, and then include a second step that merges highly ranked adjacent 
unigrams to form multi-word phrases. This has a nice side effect of 
allowing us to produce keyphrases of arbitrary length. For example, if 
we rank unigrams and find that "advanced", "natural", "language", and 
"processing" all get high ranks, then we would look at the original text
 and see that these words appear consecutively and create a final 
keyphrase using all four together. Note that the unigrams placed in the 
graph can be filtered by part of speech. The authors found that 
adjectives and nouns were the best to include. Thus, some linguistic 
knowledge comes into play in this step.</p>
<p>Edges are created based on word <a href="https://en.wikipedia.org/wiki/Co-occurrence" title="Co-occurrence">co-occurrence</a> in this application of TextRank. Two vertices are connected by an edge if the <a href="https://en.wikipedia.org/wiki/Unigram" class="mw-redirect" title="Unigram">unigrams</a>
 appear within a window of size N in the original text. N is typically 
around 210. Thus, "natural" and "language" might be linked in a text 
about NLP. "Natural" and "processing" would also be linked because they 
would both appear in the same string of N words. These edges build on 
the notion of "text <a href="https://en.wikipedia.org/wiki/Cohesion_%28linguistics%29" title="Cohesion (linguistics)">cohesion</a>"
 and the idea that words that appear near each other are likely related 
in a meaningful way and "recommend" each other to the reader.</p>
<p>Since this method simply ranks the individual vertices, we need a way
 to threshold or produce a limited number of keyphrases. The technique 
chosen is to set a count T to be a user-specified fraction of the total 
number of vertices in the graph. Then the top T vertices/unigrams are 
selected based on their stationary probabilities. A post- processing 
step is then applied to merge adjacent instances of these T unigrams. As
 a result, potentially more or less than T final keyphrases will be 
produced, but the number should be roughly proportional to the length of
 the original text.</p>
<p>It is not initially clear why applying PageRank to a co-occurrence 
graph would produce useful keyphrases. One way to think about it is the 
following. A word that appears multiple times throughout a text may have
 many different co-occurring neighbors. For example, in a text about 
machine learning, the unigram "learning" might co-occur with "machine", 
"supervised", "un-supervised", and "semi-supervised" in four different 
sentences. Thus, the "learning" vertex would be a central "hub" that 
connects to these other modifying words. Running PageRank/TextRank on 
the graph is likely to rank "learning" highly. Similarly, if the text 
contains the phrase "supervised classification", then there would be an 
edge between "supervised" and "classification". If "classification" 
appears several other places and thus has many neighbors, its importance
 would contribute to the importance of "supervised". If it ends up with a
 high rank, it will be selected as one of the top T unigrams, along with
 "learning" and probably "classification". In the final post-processing 
step, we would then end up with keyphrases "supervised learning" and 
"supervised classification".</p>
<p>In short, the co-occurrence graph will contain densely connected 
regions for terms that appear often and in different contexts. A random 
walk on this graph will have a stationary distribution that assigns 
large probabilities to the terms in the centers of the clusters. This is
 similar to densely connected Web pages getting ranked highly by 
PageRank. This approach has also been used in document summarization, 
considered below.</p>
<h3><span class="mw-headline" id="Document_summarization">Document summarization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=9" title="Edit section: Document summarization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Like keyphrase extraction, document summarization aims to identify 
the essence of a text. The only real difference is that now we are 
dealing with larger text unitswhole sentences instead of words and 
phrases.</p>
<p>Before getting into the details of some summarization methods, we 
will mention how summarization systems are typically evaluated. The most
 common way is using the so-called <a href="https://en.wikipedia.org/wiki/ROUGE_%28metric%29" title="ROUGE (metric)">ROUGE</a>
 (Recall-Oriented Understudy for Gisting Evaluation) measure. This is a 
recall-based measure that determines how well a system-generated summary
 covers the content present in one or more human-generated model 
summaries known as references. It is recall-based to encourage systems 
to include all the important topics in the text. Recall can be computed 
with respect to unigram, bigram, trigram, or 4-gram matching. For 
example, ROUGE-1 is computed as division of count of unigrams in 
reference that appear in system and count of unigrams in reference 
summary.</p>
<p>If there are multiple references, the ROUGE-1 scores are averaged. 
Because ROUGE is based only on content overlap, it can determine if the 
same general concepts are discussed between an automatic summary and a 
reference summary, but it cannot determine if the result is coherent or 
the sentences flow together in a sensible manner. High-order n-gram 
ROUGE measures try to judge fluency to some degree. Note that ROUGE is 
similar to the BLEU measure for machine translation, but BLEU is 
precision- based, because translation systems favor accuracy.</p>
<p>A promising line in document summarization is adaptive document/text summarization.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">[4]</a></sup>
 The idea of adaptive summarization involves preliminary recognition of 
document/text genre and subsequent application of summarization 
algorithms optimized for this genre. First summarizes that perform 
adaptive summarization have been created.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">[5]</a></sup></p>
<h4><span class="mw-headline" id="Supervised_learning_approaches_2">Supervised learning approaches</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=10" title="Edit section: Supervised learning approaches">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Supervised text summarization is very much like supervised keyphrase 
extraction. Basically, if you have a collection of documents and 
human-generated summaries for them, you can learn features of sentences 
that make them good candidates for inclusion in the summary. Features 
might include the position in the document (i.e., the first few 
sentences are probably important), the number of words in the sentence, 
etc. The main difficulty in supervised extractive summarization is that 
the known summaries must be manually created by extracting sentences so 
the sentences in an original training document can be labeled as "in 
summary" or "not in summary". This is not typically how people create 
summaries, so simply using journal abstracts or existing summaries is 
usually not sufficient. The sentences in these summaries do not 
necessarily match up with sentences in the original text, so it would be
 difficult to assign labels to examples for training. Note, however, 
that these natural summaries can still be used for evaluation purposes, 
since ROUGE-1 only cares about unigrams.</p>
<h4><span class="mw-headline" id="Maximum_entropy-based_summarization">Maximum entropy-based summarization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=11" title="Edit section: Maximum entropy-based summarization">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>During the DUC 2001 and 2002 evaluation workshops, <a href="https://en.wikipedia.org/wiki/Netherlands_Organisation_for_Applied_Scientific_Research" title="Netherlands Organisation for Applied Scientific Research">TNO</a>
 developed a sentence extraction system for multi-document summarization
 in the news domain. The system was based on a hybrid system using a <a href="https://en.wikipedia.org/wiki/Naive_Bayes" class="mw-redirect" title="Naive Bayes">naive Bayes</a>
 classifier and statistical language models for modeling salience. 
Although the system exhibited good results, the researchers wanted to 
explore the effectiveness of a <a href="https://en.wikipedia.org/wiki/Maximum_entropy_classifier" class="mw-redirect" title="Maximum entropy classifier">maximum entropy</a>
 (ME) classifier for the meeting summarization task, as ME is known to 
be robust against feature dependencies. Maximum entropy has also been 
applied successfully for summarization in the broadcast news domain.</p>
<h4><span class="mw-headline" id="TextRank_and_LexRank">TextRank and LexRank</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=12" title="Edit section: TextRank and LexRank">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>The unsupervised approach to summarization is also quite similar in 
spirit to unsupervised keyphrase extraction and gets around the issue of
 costly training data. Some unsupervised summarization approaches are 
based on finding a "<a href="https://en.wikipedia.org/wiki/Centroid" title="Centroid">centroid</a>"
 sentence, which is the mean word vector of all the sentences in the 
document. Then the sentences can be ranked with regard to their 
similarity to this centroid sentence.</p>
<p>A more principled way to estimate sentence importance is using random walks and eigenvector centrality. LexRank<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">[6]</a></sup>
 is an algorithm essentially identical to TextRank, and both use this 
approach for document summarization. The two methods were developed by 
different groups at the same time, and LexRank simply focused on 
summarization, but could just as easily be used for keyphrase extraction
 or any other NLP ranking task.</p>
<p>In both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document.</p>
<p>The edges between sentences are based on some form of semantic similarity or content overlap. While LexRank uses <a href="https://en.wikipedia.org/wiki/Cosine_similarity" title="Cosine similarity">cosine similarity</a> of <a href="https://en.wikipedia.org/wiki/TF-IDF" class="mw-redirect" title="TF-IDF">TF-IDF</a> vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (<a href="https://en.wikipedia.org/wiki/Quantile_normalization" title="Quantile normalization">normalized</a>
 by the sentences' lengths). The LexRank paper explored using unweighted
 edges after applying a threshold to the cosine values, but also 
experimented with using edges with weights equal to the similarity 
score. TextRank uses continuous <a href="https://en.wikipedia.org/wiki/Similarity_score" title="Similarity score">similarity scores</a> as weights.</p>
<p>In both algorithms, the sentences are ranked by applying PageRank to 
the resulting graph. A summary is formed by combining the top ranking 
sentences, using a threshold or length cutoff to limit the size of the 
summary.</p>
<p>It is worth noting that TextRank was applied to summarization exactly
 as described here, while LexRank was used as part of a larger 
summarization system (<a href="https://en.wikipedia.org/w/index.php?title=MEAD&amp;action=edit&amp;redlink=1" class="new" title="MEAD (page does not exist)">MEAD</a>)
 that combines the LexRank score (stationary probability) with other 
features like sentence position and length using a linear combination 
with either user-specified or automatically tuned weights. In this case,
 some training documents might be needed, though the TextRank results 
show the additional features are not absolutely necessary.</p>
<p>Another important distinction is that TextRank was used for single 
document summarization, while LexRank has been applied to multi-document
 summarization. The task remains the same in both casesonly the number 
of sentences to choose from has grown. However, when summarizing 
multiple documents, there is a greater risk of selecting duplicate or 
highly redundant sentences to place in the same summary. Imagine you 
have a cluster of news articles on a particular event, and you want to 
produce one summary. Each article is likely to have many similar 
sentences, and you would only want to include distinct ideas in the 
summary. To address this issue, LexRank applies a heuristic 
post-processing step that builds up a summary by adding sentences in 
rank order, but discards any sentences that are too similar to ones 
already placed in the summary. The method used is called Cross-Sentence 
Information Subsumption (CSIS).</p>
<p>These methods work based on the idea that sentences "recommend" other
 similar sentences to the reader. Thus, if one sentence is very similar 
to many others, it will likely be a sentence of great importance. The 
importance of this sentence also stems from the importance of the 
sentences "recommending" it. Thus, to get ranked highly and placed in a 
summary, a sentence must be similar to many sentences that are in turn 
also similar to many other sentences. This makes intuitive sense and 
allows the algorithms to be applied to any arbitrary new text. The 
methods are domain-independent and easily portable. One could imagine 
the features indicating important sentences in the news domain might 
vary considerably from the biomedical domain. However, the unsupervised 
"recommendation"-based approach applies to any domain.</p>
<h4><span class="mw-headline" id="Multi-document_summarization">Multi-document summarization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=13" title="Edit section: Multi-document summarization">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<div role="note" class="hatnote">Main article: <a href="https://en.wikipedia.org/wiki/Multi-document_summarization" title="Multi-document summarization">Multi-document summarization</a></div>
<p><b>Multi-document summarization</b> is an automatic procedure aimed 
at extraction of information from multiple texts written about the same 
topic. Resulting summary report allows individual users, such as 
professional information consumers, to quickly familiarize themselves 
with information contained in a large cluster of documents. In such a 
way, multi-document summarization systems are complementing the <a href="https://en.wikipedia.org/wiki/News_aggregators" class="mw-redirect" title="News aggregators">news aggregators</a> performing the next step down the road of coping with <a href="https://en.wikipedia.org/wiki/Information_overload" title="Information overload">information overload</a>. Multi-document summarization may also be done in response to a question.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">[7]</a></sup></p>
<p>Multi-document summarization creates information reports that are 
both concise and comprehensive. With different opinions being put 
together and outlined, every topic is described from multiple 
perspectives within a single document. While the goal of a brief summary
 is to simplify information search and cut the time by pointing to the 
most relevant source documents, comprehensive multi-document summary 
should itself contain the required information, hence limiting the need 
for accessing original files to cases when refinement is required. 
Automatic summaries present information extracted from multiple sources 
algorithmically, without any editorial touch or subjective human 
intervention, thus making it completely unbiased.</p>
<h5><span class="mw-headline" id="Incorporating_diversity">Incorporating diversity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=14" title="Edit section: Incorporating diversity">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>Multi-document extractive summarization faces a problem of potential 
redundancy. Ideally, we would like to extract sentences that are both 
"central" (i.e., contain the main ideas) and "diverse" (i.e., they 
differ from one another). LexRank deals with diversity as a heuristic 
final stage using CSIS, and other systems have used similar methods, 
such as Maximal Marginal Relevance (MMR),<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">[8]</a></sup>
 in trying to eliminate redundancy in information retrieval results. 
There is a general purpose graph-based ranking algorithm like 
Page/Lex/TextRank that handles both "centrality" and "diversity" in a 
unified mathematical framework based on <a href="https://en.wikipedia.org/wiki/Absorbing_Markov_chain" title="Absorbing Markov chain">absorbing Markov chain</a>
 random walks. (An absorbing random walk is like a standard random walk,
 except some states are now absorbing states that act as "black holes" 
that cause the walk to end abruptly at that state.) The algorithm is 
called GRASSHOPPER.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">[9]</a></sup>
 In addition to explicitly promoting diversity during the ranking 
process, GRASSHOPPER incorporates a prior ranking (based on sentence 
position in the case of summarization).</p>
<p>The state of the art results for multi-document summarization, 
however, are obtained using mixtures of submodular functions. These 
methods have achieved the state of the art results for Document 
Summarization Corpora, DUC 04 - 07.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">[10]</a></sup>
 Similar results were also achieved with the use of determinantal point 
processes (which are a special case of submodular functions) for DUC-04.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">[11]</a></sup></p>
<p>A new method for multi-lingual multi-document summarization that 
avoids redundancy works by simplifying and generating ideograms that 
represent the meaning of each sentence in each document and then 
evaluates similarity "qualitatively" by comparing the shape and position
 of said ideograms has recently been developed. This tool does not use 
word frequency, does not need training or preprocessing of any kind and 
works by generating ideograms that represent the meaning of each 
sentence and then summarizes using two user-supplied parameters: 
equivalence (when are two sentences to be considered equivalent) and 
relevance (how long is the desired summary). The Simplish Simplifying 
&amp; Summarizing tool<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">[12]</a></sup> - performs just such an automatic multi-lingual multi-document summarization.</p>
<h3><span class="mw-headline" id="Submodular_Functions_as_generic_tools_for_summarization">Submodular Functions as generic tools for summarization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=15" title="Edit section: Submodular Functions as generic tools for summarization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The idea of a <a href="https://en.wikipedia.org/wiki/Submodular_set_function" title="Submodular set function">Submodular set function</a>
 has recently emerged as a powerful modeling tool for various 
summarization problems. Submodular functions naturally model notions of <i>coverage</i>, <i>information</i>, <i>representation</i> and <i>diversity</i>.
 Moreover, several important combinatorial optimization problems occur 
as special instances of submodular optimization. For example, the <a href="https://en.wikipedia.org/wiki/Set_cover_problem" title="Set cover problem">set cover problem</a>
 is a special case of submodular optimization, since the set cover 
function is submodular. The set cover function attempts to find a subset
 of objects which <i>cover</i> a given set of concepts. For example, in 
document summarization, one would like the summary to cover all 
important and relevant concepts in the document. This is an instance of 
set cover. Similarly, the <a href="https://en.wikipedia.org/wiki/Facility_location_problem" title="Facility location problem">facility location problem</a>
 is a special case of submodular functions. The Facility Location 
function also naturally models coverage and diversity. Another example 
of a submodular optimization problem is using a <a href="https://en.wikipedia.org/wiki/Determinantal_point_process" title="Determinantal point process">Determinantal point process</a>
 to model diversity. Similarly, the Maximum-Marginal-Relevance procedure
 can also be seen as an instance of submodular optimization. All these 
important models encouraging coverage, diversity and information are all
 submodular. Moreover, submodular functions can be efficiently combined 
together, and the resulting function is still submodular. Hence, one 
could combine one submodular function which models diversity, another 
one which models coverage and use human supervision to learn a right 
model of a submodular function for the problem.</p>
<p>While submodular functions are fitting problems for summarization, 
they also admit very efficient algorithms for optimization. For example,
 a simple <a href="https://en.wikipedia.org/wiki/Greedy_algorithm" title="Greedy algorithm">greedy algorithm</a> admits a constant factor guarantee.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">[13]</a></sup>
 Moreover, the greedy algorithm is extremely simple to implement and can
 scale to large datasets, which is very important for summarization 
problems.</p>
<p>Submodular functions have achieved state-of-the-art for almost all 
summarization problems. For example, work by Lin and Bilmes, 2012<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">[14]</a></sup>
 shows that submodular functions achieve the best results to date on 
DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization. 
Similarly, work by Lin and Bilmes, 2011,<sup id="cite_ref-15" class="reference"><a href="#cite_note-15">[15]</a></sup>
 shows that many existing systems for automatic summarization are 
instances of submodular functions. This was a break through result 
establishing submodular functions as the right models for summarization 
problems.</p>
<p>Submodular Functions have also been used for other summarization tasks. Tschiatschek et al., 2014 show<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">[16]</a></sup>
 that mixtures of submodular functions achieve state-of-the-art results 
for image collection summarization. Similarly, Bairi et al., 2015<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">[17]</a></sup>
 show the utility of submodular functions for summarizing multi-document
 topic hierarchies. Submodular Functions have also successfully been 
used for summarizing machine learning datasets.<sup id="cite_ref-18" class="reference"><a href="#cite_note-18">[18]</a></sup></p>
<h3><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=16" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<table class="plainlinks metadata ambox mbox-small-left ambox-content" role="presentation">
<tbody><tr>
<td class="mbox-image"><a href="https://en.wikipedia.org/wiki/File:Wiki_letter_w_cropped.svg" class="image"><img alt="[icon]" src="Automatic%20summarization%20-%20Wikipedia_files/20px-Wiki_letter_w_cropped.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Wiki_letter_w_cropped.svg/30px-Wiki_letter_w_cropped.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Wiki_letter_w_cropped.svg/40px-Wiki_letter_w_cropped.svg.png 2x" data-file-width="44" data-file-height="31" width="20" height="14"></a></td>
<td class="mbox-text"><span class="mbox-text-span"><b>This section needs expansion</b>. <small>You can help by <a class="external text" href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=">adding to it</a>.</small> <small><i>(February 2017)</i></small></span></td>
</tr>
</tbody></table>
<p>Specific applications of automatic summarization include:</p>
<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Reddit" title="Reddit">reddit</a> <a href="https://en.wikipedia.org/wiki/Internet_bot" title="Internet bot">bot</a> "autotldr",<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">[19]</a></sup>
 created in 2011 summarizes news articles in the comment-section of 
reddit posts. It was found to be very useful by the reddit community 
which upvoted its summaries hundreds of thousands of times.<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">[20]</a></sup> The name is reference to <a href="https://en.wikipedia.org/wiki/TL;DR" title="TL;DR">TL;DR</a>  <a href="https://en.wikipedia.org/wiki/Internet_slang" title="Internet slang">Internet slang</a> for "too long; didn't read".<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">[21]</a></sup><sup id="cite_ref-22" class="reference"><a href="#cite_note-22">[22]</a></sup></li>
</ul>
<h2><span class="mw-headline" id="Evaluation_techniques">Evaluation techniques</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=17" title="Edit section: Evaluation techniques">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.</p>
<p>Evaluation techniques fall into intrinsic and extrinsic,<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">[23]</a></sup> inter-textual and intra-textual.<sup id="cite_ref-24" class="reference"><a href="#cite_note-24">[24]</a></sup></p>
<h3><span class="mw-headline" id="Intrinsic_and_extrinsic_evaluation">Intrinsic and extrinsic evaluation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=18" title="Edit section: Intrinsic and extrinsic evaluation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>An intrinsic evaluation tests the summarization system in and of 
itself while an extrinsic evaluation tests the summarization based on 
how it affects the completion of some other task. Intrinsic evaluations 
have assessed mainly the coherence and informativeness of summaries. 
Extrinsic evaluations, on the other hand, have tested the impact of 
summarization on tasks like relevance assessment, reading comprehension,
 etc.</p>
<h3><span class="mw-headline" id="Inter-textual_and_intra-textual">Inter-textual and intra-textual</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=19" title="Edit section: Inter-textual and intra-textual">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Intra-textual methods assess the output of a specific summarization 
system, and the inter-textual ones focus on contrastive analysis of 
outputs of several summarization systems.</p>
<p>Human judgement often has wide variance on what is considered a 
"good" summary, which means that making the evaluation process automatic
 is particularly difficult. Manual evaluation can be used, but this is 
both time and labor-intensive as it requires humans to read not only the
 summaries but also the source documents. Other issues are those 
concerning <a href="https://en.wikipedia.org/wiki/Coherence_%28linguistics%29" title="Coherence (linguistics)">coherence</a> and coverage.</p>
<p>One of the metrics used in <a href="https://en.wikipedia.org/wiki/NIST" class="mw-redirect" title="NIST">NIST</a>'s
 annual Document Understanding Conferences, in which research groups 
submit their systems for both summarization and translation tasks, is 
the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation <a rel="nofollow" class="external autonumber" href="http://haydn.isi.edu/ROUGE/">[2]</a>). It essentially calculates <a href="https://en.wikipedia.org/wiki/N-gram" title="N-gram">n-gram</a>
 overlaps between automatically generated summaries and 
previously-written human summaries. A high level of overlap should 
indicate a high level of shared concepts between the two summaries. Note
 that overlap metrics like this are unable to provide any feedback on a 
summary's coherence. <a href="https://en.wikipedia.org/wiki/Anaphora_%28linguistics%29" title="Anaphora (linguistics)">Anaphor resolution</a>
 remains another problem yet to be fully solved. Similarly, for image 
summarization, Tschiatschek et al., developed a Visual-ROUGE score which
 judges the performance of algorithms for image summarization.<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">[25]</a></sup></p>
<h3><span class="mw-headline" id="Domain_specific_versus_domain_independent_summarization_techniques">Domain specific versus domain independent summarization techniques</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=20" title="Edit section: Domain specific versus domain independent summarization techniques">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Domain independent summarization techniques generally apply sets of 
general features which can be used to identify information-rich text 
segments. Recent research focus has drifted to domain-specific 
summarization techniques that utilize the available knowledge specific 
to the domain of text. For example, automatic summarization research on 
medical text generally attempts to utilize the various sources of 
codified medical knowledge and ontologies.<sup id="cite_ref-26" class="reference"><a href="#cite_note-26">[26]</a></sup></p>
<h3><span class="mw-headline" id="Evaluating_summaries_qualitatively">Evaluating summaries qualitatively</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=21" title="Edit section: Evaluating summaries qualitatively">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The main drawback of the evaluation systems existing so far is that 
we need at least one reference summary, and for some methods more than 
one, to be able to compare automatic summaries with models. This is a 
hard and expensive task. Much effort has to be done in order to have 
corpus of texts and their corresponding summaries. Furthermore, for some
 methods, not only do we need to have human-made summaries available for
 comparison, but also manual annotation has to be performed in some of 
them (e.g. SCU in the Pyramid Method). In any case, what the evaluation 
methods need as an input, is a set of summaries to serve as gold 
standards and a set of automatic summaries. Moreover, they all perform a
 quantitative evaluation with regard to different similarity metrics.</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=22" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Sentence_extraction" title="Sentence extraction">Sentence extraction</a></li>
<li><a href="https://en.wikipedia.org/wiki/Text_mining" title="Text mining">Text mining</a></li>
<li><a href="https://en.wikipedia.org/wiki/Multi-document_summarization" title="Multi-document summarization">Multi-document summarization</a></li>
</ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=23" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-count references-column-count-2" style="-moz-column-count: 2; -webkit-column-count: 2; column-count: 2; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Jorge
 E. Camargo and Fabio A. Gonzlez. A Multi-class Kernel Alignment Method
 for Image Collection Summarization. In Proceedings of the 14th 
Iberoamerican Conference on Pattern Recognition: Progress in Pattern 
Recognition, Image Analysis, Computer Vision, and Applications (CIARP 
'09), Eduardo Bayro-Corrochano and Jan-Olof Eklundh (Eds.). 
Springer-Verlag, Berlin, Heidelberg, 545-552. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007%2F978-3-642-10268-4_64">10.1007/978-3-642-10268-4_64</a></span></li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="http://arxiv.org/pdf/cs/0212020.pdf">http://arxiv.org/pdf/cs/0212020.pdf</a></span></li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Rada Mihalcea and Paul Tarau, 2004: <i>TextRank: Bringing Order into Texts</i>, Department of Computer Science University of North Texas <cite class="citation web"><a rel="nofollow" class="external text" href="https://web.archive.org/web/20120617170501/http://acl.ldc.upenn.edu/acl2004/emnlp/pdf/Mihalcea.pdf">"Archived copy"</a> <span style="font-size:85%;">(PDF)</span>. Archived from <a rel="nofollow" class="external text" href="http://acl.ldc.upenn.edu/acl2004/emnlp/pdf/Mihalcea.pdf">the original</a> <span style="font-size:85%;">(PDF)</span> on 2012-06-17<span class="reference-accessdate">. Retrieved <span class="nowrap">2012-07-20</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.btitle=Archived+copy&amp;rft.genre=unknown&amp;rft_id=http%3A%2F%2Facl.ldc.upenn.edu%2Facl2004%2Femnlp%2Fpdf%2FMihalcea.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Yatsko,
 V. et al Automatic genre recognition and adaptive text summarization. 
In: Automatic Documentation and Mathematical Linguistics, 2010, Volume 
44, Number 3, pp.111-120.</span></li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://yatsko.zohosites.com/universal-summarizer-unis.html">UNIS (Universal Summarizer)</a></span></li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Gne Erkan and Dragomir R. Radev: <i>LexRank: Graph-based Lexical Centrality as Salience in Text Summarization <a rel="nofollow" class="external autonumber" href="http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html">[1]</a></i></span></li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">"<a rel="nofollow" class="external text" href="https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis">Versatile question answering systems: seeing in synthesis</a>", Int. J. of Intelligent Information Database Systems, 5(2), 119-142, 2011.</span></li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Carbonell,
 Jaime, and Jade Goldstein. "The use of MMR, diversity-based reranking 
for reordering documents and producing summaries." Proceedings of the 
21st annual international ACM SIGIR conference on Research and 
development in information retrieval. ACM, 1998.</span></li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Zhu, Xiaojin, et al. "Improving Diversity in Ranking using Absorbing Random Walks." HLT-NAACL. 2007.</span></li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Hui Lin, Jeff Bilmes. "Learning mixtures of submodular shells with application to document summarization</span></li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Alex
 Kulesza and Ben Taskar, Determinantal point processes for machine 
learning. Foundations and Trends in Machine Learning, December 2012.</span></li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://simplish.org/">"Simplish Simplification and Summarization Tool"</a>. The Goodwill Consortium<span class="reference-accessdate">. Retrieved <span class="nowrap">February 8,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.btitle=Simplish+Simplification+and+Summarization+Tool&amp;rft.genre=unknown&amp;rft_id=http%3A%2F%2Fsimplish.org%2F&amp;rft.pub=The+Goodwill+Consortium&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Nemhauser,
 George L., Laurence A. Wolsey, and Marshall L. Fisher. "An analysis of 
approximations for maximizing submodular set functionsI." Mathematical 
Programming 14.1 (1978): 265-294.</span></li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Hui Lin, Jeff Bilmes. "Learning mixtures of submodular shells with application to document summarization", UAI, 2012</span></li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Hui
 Lin, Jeff Bilmes. "A Class of Submodular Functions for Document 
Summarization", The 49th Annual Meeting of the Association for 
Computational Linguistics: Human Language Technologies (ACL-HLT) , 2011</span></li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Sebastian
 Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning 
Mixtures of Submodular Functions for Image Collection Summarization, In 
Advances of Neural Information Processing Systems (NIPS), Montreal, 
Canada, December - 2014.</span></li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Ramakrishna
 Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, Summarizing 
Multi-Document Topic Hierarchies using Submodular Mixtures, To Appear In
 the Annual Meeting of the Association for Computational Linguistics 
(ACL), Beijing, China, July - 2015</span></li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Kai
 Wei, Rishabh Iyer, and Jeff Bilmes, Submodularity in Data Subset 
Selection and Active Learning, To Appear In Proc. International 
Conference on Machine Learning (ICML), Lille, France, June - 2015</span></li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.reddit.com/user/autotldr">"overview for autotldr"</a>. <i>reddit</i><span class="reference-accessdate">. Retrieved <span class="nowrap">9 February</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.atitle=overview+for+autotldr&amp;rft.genre=unknown&amp;rft_id=https%3A%2F%2Fwww.reddit.com%2Fuser%2Fautotldr&amp;rft.jtitle=reddit&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation book">Squire, Megan. <a rel="nofollow" class="external text" href="https://books.google.de/books?id=_qXWDQAAQBAJ&amp;pg=PA185"><i>Mastering Data Mining with Python  Find patterns hidden in your data</i></a>. Packt Publishing Ltd. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/9781785885914" title="Special:BookSources/9781785885914">9781785885914</a><span class="reference-accessdate">. Retrieved <span class="nowrap">9 February</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Megan&amp;rft.aulast=Squire&amp;rft.btitle=Mastering+Data+Mining+with+Python+%93+Find+patterns+hidden+in+your+data&amp;rft.genre=book&amp;rft_id=https%3A%2F%2Fbooks.google.de%2Fbooks%3Fid%3D_qXWDQAAQBAJ%26pg%3DPA185&amp;rft.isbn=9781785885914&amp;rft.pub=Packt+Publishing+Ltd&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.lifewire.com/what-is-tldr-2483633">"What Is 'TLDR'?"</a>. <i>Lifewire</i><span class="reference-accessdate">. Retrieved <span class="nowrap">9 February</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.atitle=What+Is+%27TLDR%27%3F&amp;rft.genre=unknown&amp;rft_id=https%3A%2F%2Fwww.lifewire.com%2Fwhat-is-tldr-2483633&amp;rft.jtitle=Lifewire&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.ibtimes.com/g00/what-does-tldr-mean-ama-til-glossary-reddit-terms-abbreviations-431704?i10c">"What Does TL;DR Mean? AMA? TIL? Glossary Of Reddit Terms And Abbreviations"</a>. International Business Times. 29 March 2012<span class="reference-accessdate">. Retrieved <span class="nowrap">9 February</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.btitle=What+Does+TL%3BDR+Mean%3F+AMA%3F+TIL%3F+Glossary+Of+Reddit+Terms+And+Abbreviations&amp;rft.date=2012-03-29&amp;rft.genre=unknown&amp;rft_id=http%3A%2F%2Fwww.ibtimes.com%2Fg00%2Fwhat-does-tldr-mean-ama-til-glossary-reddit-terms-abbreviations-431704%3Fi10c&amp;rft.pub=International+Business+Times&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings2/sum-mani.pdf">Mani, I. Summarization evaluation: an overview</a></span></li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Yatsko
 V. A., Vishnyakov T. N. A method for evaluating modern systems of 
automatic text summarization. In: Automatic Documentation and 
Mathematical Linguistics. - 2007. - V. 41. - No 3. - P. 93-103.</span></li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Sebastian
 Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning 
Mixtures of Submodular Functions for Image Collection Summarization, In 
Advances of Neural Information Processing Systems (NIPS), Montreal, 
Canada, December - 2014. (PDF)</span></li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Sarker, Abeed; Molla, Diego; Paris, Cecile (2013). <a rel="nofollow" class="external text" href="http://link.springer.com/chapter/10.1007%2F978-3-642-38326-7_41">"An Approach for Query-focused Text Summarization for Evidence-based medicine"</a>. <i>Lecture Notes in Computer Science</i>. <b>7885</b>: 295304. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007%2F978-3-642-38326-7_41">10.1007/978-3-642-38326-7_41</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.atitle=An+Approach+for+Query-focused+Text+Summarization+for+Evidence-based+medicine&amp;rft.aufirst=Abeed&amp;rft.aulast=Sarker&amp;rft.au=Molla%2C+Diego&amp;rft.au=Paris%2C+Cecile&amp;rft.date=2013&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%252F978-3-642-38326-7_41&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-38326-7_41&amp;rft.jtitle=Lecture+Notes+in+Computer+Science&amp;rft.pages=295-304&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=7885" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
</ol>
</div>
<h3><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit&amp;section=24" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul>
<li><cite class="citation book">Hercules, Dalianis; et al. (2003). <i>Porting and evaluation of automatic summarization</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Dalianis&amp;rft.aulast=Hercules&amp;rft.btitle=Porting+and+evaluation+of+automatic+summarization&amp;rft.date=2003&amp;rft.genre=book&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span> <span class="citation-comment" style="display:none; color:#33aa33">CS1 maint: Explicit use of et al. (<a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Explicit_use_of_et_al." title="Category:CS1 maint: Explicit use of et al.">link</a>)</span></li>
<li><cite class="citation book">Roxana, Angheluta (2002). <i>The Use of Topic Segmentation for Automatic Summarization</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Angheluta&amp;rft.aulast=Roxana&amp;rft.btitle=The+Use+of+Topic+Segmentation+for+Automatic+Summarization&amp;rft.date=2002&amp;rft.genre=book&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></li>
<li><cite class="citation book">Anne, Buist (2004). <i>Automatic Summarization of Meeting Data: A Feasibility Study</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Buist&amp;rft.aulast=Anne&amp;rft.btitle=Automatic+Summarization+of+Meeting+Data%3A+A+Feasibility+Study&amp;rft.date=2004&amp;rft.genre=book&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></li>
<li><cite class="citation book">Annie, Louis (2009). <i>Performance Confidence Estimation for Automatic Summarization</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Louis&amp;rft.aulast=Annie&amp;rft.btitle=Performance+Confidence+Estimation+for+Automatic+Summarization&amp;rft.date=2009&amp;rft.genre=book&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></li>
<li><cite class="citation book">Elena, Lloret and Manuel, Palomar (2009). <i>Challenging Issues of Automatic Summarization: Relevance Detection and Quality-based Evaluation</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Lloret+and+Manuel%2C+Palomar&amp;rft.aulast=Elena&amp;rft.btitle=Challenging+Issues+of+Automatic+Summarization%3A+Relevance+Detection+and+Quality-based+Evaluation&amp;rft.date=2009&amp;rft.genre=book&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></li>
<li><cite class="citation book">Andrew, Goldberg (2007). <i>Automatic Summarization</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Goldberg&amp;rft.aulast=Andrew&amp;rft.btitle=Automatic+Summarization&amp;rft.date=2007&amp;rft.genre=book&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></li>
<li><cite class="citation book">Endres-Niggemeyer, Brigitte (1998). <i>Summarizing Information</i>. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/3-540-63735-4" title="Special:BookSources/3-540-63735-4">3-540-63735-4</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Brigitte&amp;rft.aulast=Endres-Niggemeyer&amp;rft.btitle=Summarizing+Information&amp;rft.date=1998&amp;rft.genre=book&amp;rft.isbn=3-540-63735-4&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></li>
<li><cite class="citation book">Marcu, Daniel (2000). <i>The Theory and Practice of Discourse Parsing and Summarization</i>. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-262-13372-5" title="Special:BookSources/0-262-13372-5">0-262-13372-5</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Daniel&amp;rft.aulast=Marcu&amp;rft.btitle=The+Theory+and+Practice+of+Discourse+Parsing+and+Summarization&amp;rft.date=2000&amp;rft.genre=book&amp;rft.isbn=0-262-13372-5&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></li>
<li><cite class="citation book">Mani, Inderjeet (2001). <i>Automatic Summarization</i>. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/1-58811-060-5" title="Special:BookSources/1-58811-060-5">1-58811-060-5</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Inderjeet&amp;rft.aulast=Mani&amp;rft.btitle=Automatic+Summarization&amp;rft.date=2001&amp;rft.genre=book&amp;rft.isbn=1-58811-060-5&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></li>
<li><cite class="citation book">Huff, Jason (2010). <a rel="nofollow" class="external text" href="http://www.jason-huff.com/projects/autosummarize/"><i>AutoSummarize</i></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Jason&amp;rft.aulast=Huff&amp;rft.btitle=AutoSummarize&amp;rft.date=2010&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.jason-huff.com%2Fprojects%2Fautosummarize%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span>, Conceptual artwork using automatic summarization software in Microsoft Word 2008.</li>
<li><cite class="citation book">Lehmam, Abderrafih (2010). <a rel="nofollow" class="external text" href="http://portal.acm.org/citation.cfm?id=1937055.1937111&amp;coll=DL&amp;dl=GUIDE&amp;CFID=23185814&amp;CFTOKEN=40272014/"><i>Essential summarizer: innovative automatic text summarization software in twenty languages - ACM Digital Library</i></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Abderrafih&amp;rft.aulast=Lehmam&amp;rft.btitle=Essential+summarizer%3A+innovative+automatic+text+summarization+software+in+twenty+languages+-+ACM+Digital+Library&amp;rft.date=2010&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fportal.acm.org%2Fcitation.cfm%3Fid%3D1937055.1937111%26coll%3DDL%26dl%3DGUIDE%26CFID%3D23185814%26CFTOKEN%3D40272014%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span>, Published in Proceeding RIAO'10 Adaptivity, Personalization and Fusion of Heterogeneous Information, CID Paris, France</li>
<li><cite class="citation book">Xiaojin, Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski (2007). <a rel="nofollow" class="external text" href="http://pages.cs.wisc.edu/%7Ejerryzhu/pub/grasshopper.pdf"><i>Improving diversity in ranking using absorbing random walks</i></a> <span style="font-size:85%;">(PDF)</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Zhu%2C+Andrew+Goldberg%2C+Jurgen+Van+Gael%2C+and+David+Andrzejewski&amp;rft.aulast=Xiaojin&amp;rft.btitle=Improving+diversity+in+ranking+using+absorbing+random+walks&amp;rft.date=2007&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fpages.cs.wisc.edu%2F~jerryzhu%2Fpub%2Fgrasshopper.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span>, The GRASSHOPPER algorithm</li>
<li><cite class="citation book">Miranda-Jimnez, Sabino, Gelbukh, Alexander, and Sidorov, Grigori (2013). <a rel="nofollow" class="external text" href="http://link.springer.com/chapter/10.1007%2F978-3-642-35786-2_18"><i>Summarizing Conceptual Graphs for Automatic Summarization Task</i></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Sabino%2C+Gelbukh%2C+Alexander%2C+and+Sidorov%2C+Grigori&amp;rft.aulast=Miranda-Jim%C3%A9nez&amp;rft.btitle=Summarizing+Conceptual+Graphs+for+Automatic+Summarization+Task&amp;rft.date=2013&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%252F978-3-642-35786-2_18&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span>, Conceptual Structures for STEM Research and Education.</li>
<li><cite class="citation book">Miranda-Jimnez, Sabino, Gelbukh, Alexander, and Sidorov, Grigori (2014). <a rel="nofollow" class="external text" href="http://www.igi-global.com/article/conceptual-graphs-as-framework-for-summarizing-short-texts/134888"><i>Conceptual Graphs as Framework for Summarizing Short Texts Task</i></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomatic+summarization&amp;rft.aufirst=Sabino%2C+Gelbukh%2C+Alexander%2C+and+Sidorov%2C+Grigori&amp;rft.aulast=Miranda-Jim%C3%A9nez&amp;rft.btitle=Conceptual+Graphs+as+Framework+for+Summarizing+Short+Texts+Task&amp;rft.date=2014&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.igi-global.com%2Farticle%2Fconceptual-graphs-as-framework-for-summarizing-short-texts%2F134888&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span>, International Journal of Conceptual Structures and Smart Applications.</li>
</ul>


<!-- 
NewPP limit report
Parsed by mw1197
Cached time: 20170409053941
Cache expiry: 2592000
Dynamic content: false
CPU time usage: 0.320 seconds
Real time usage: 0.381 seconds
Preprocessor visited node count: 1219/1000000
Preprocessor generated node count: 0/1500000
Postexpand include size: 50476/2097152 bytes
Template argument size: 5908/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 3/500
Lua time usage: 0.153/10.000 seconds
Lua memory usage: 4.3 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  313.576      1 -total
 41.03%  128.661      1 Template:Reflist
 34.09%  106.904      4 Template:Ambox
 26.52%   83.154      1 Template:Multiple_issues
 23.13%   72.527      5 Template:Cite_web
 20.13%   63.124     15 Template:Cite_book
  9.81%   30.765      1 Template:More_footnotes
  3.72%   11.669      1 Template:Main_article
  3.39%   10.644      1 Template:Expand_section
  3.24%   10.163      1 Template:DOI
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:637199-0!*!0!!en!4!* and timestamp 20170409053943 and revision id 774545866
 -->
<noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;oldid=774545866">https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;oldid=774545866</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="https://en.wikipedia.org/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="https://en.wikipedia.org/wiki/Category:Computational_linguistics" title="Category:Computational linguistics">Computational linguistics</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Natural_language_processing" title="Category:Natural language processing">Natural language processing</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Tasks_of_natural_language_processing" title="Category:Tasks of natural language processing">Tasks of natural language processing</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Data_mining" title="Category:Data mining">Data mining</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="https://en.wikipedia.org/wiki/Category:Articles_lacking_in-text_citations_from_March_2015" title="Category:Articles lacking in-text citations from March 2015">Articles lacking in-text citations from March 2015</a></li><li><a href="https://en.wikipedia.org/wiki/Category:All_articles_lacking_in-text_citations" title="Category:All articles lacking in-text citations">All articles lacking in-text citations</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_articles_needing_style_editing_from_March_2015" title="Category:Wikipedia articles needing style editing from March 2015">Wikipedia articles needing style editing from March 2015</a></li><li><a href="https://en.wikipedia.org/wiki/Category:All_articles_needing_style_editing" title="Category:All articles needing style editing">All articles needing style editing</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_to_be_expanded_from_February_2017" title="Category:Articles to be expanded from February 2017">Articles to be expanded from February 2017</a></li><li><a href="https://en.wikipedia.org/wiki/Category:All_articles_to_be_expanded" title="Category:All articles to be expanded">All articles to be expanded</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_using_small_message_boxes" title="Category:Articles using small message boxes">Articles using small message boxes</a></li><li><a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Explicit_use_of_et_al." title="Category:CS1 maint: Explicit use of et al.">CS1 maint: Explicit use of et al.</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>

			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="https://en.wikipedia.org/wiki/Special:MyTalk" title="Discussion about edits from this IP address [Alt+Shift+n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="https://en.wikipedia.org/wiki/Special:MyContributions" title="A list of edits made from this IP address [Alt+Shift+y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&amp;returnto=Automatic+summarization" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="https://en.wikipedia.org/w/index.php?title=Special:UserLogin&amp;returnto=Automatic+summarization" title="You're encouraged to log in; however, it's not mandatory. [Alt+Shift+o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
															<li id="ca-nstab-main" class="selected"><span><a href="https://en.wikipedia.org/wiki/Automatic_summarization" title="View the content page [Alt+Shift+c]" accesskey="c">Article</a></span></li>
															<li id="ca-talk"><span><a href="https://en.wikipedia.org/wiki/Talk:Automatic_summarization" title="Discussion about the content page [Alt+Shift+t]" accesskey="t" rel="discussion">Talk</a></span></li>
													</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<h3 id="p-variants-label" tabindex="0">
							<span>Variants</span><a href="#" tabindex="-1"></a>
						</h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
															<li id="ca-view" class="selected"><span><a href="https://en.wikipedia.org/wiki/Automatic_summarization">Read</a></span></li>
															<li id="ca-edit"><span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=edit" title="Edit this page [Alt+Shift+e]" accesskey="e">Edit</a></span></li>
															<li id="ca-history" class="collapsible"><span><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=history" title="Past revisions of this page [Alt+Shift+h]" accesskey="h">View history</a></span></li>
													</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label" style="">
						<h3 id="p-cactions-label" tabindex="0"><span>More</span><a href="#" tabindex="-1"></a></h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>

						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
							<input name="search" placeholder="Search Wikipedia" title="Search Wikipedia [Alt+Shift+f]" accesskey="f" id="searchInput" tabindex="1" autocomplete="off" type="search"><input value="Special:Search" name="title" type="hidden"><input name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton" type="submit">							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>

			<div class="body">
									<ul>
						<li id="n-mainpage-description"><a href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page [Alt+Shift+z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="https://en.wikipedia.org/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="https://en.wikipedia.org/wiki/Portal:Featured_content" title="Featured content  the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="https://en.wikipedia.org/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="https://en.wikipedia.org/wiki/Special:Random" title="Load a random article [Alt+Shift+x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="https://shop.wikimedia.org/" title="Visit the Wikipedia store">Wikipedia store</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>

			<div class="body">
									<ul>
						<li id="n-help"><a href="https://en.wikipedia.org/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="https://en.wikipedia.org/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="https://en.wikipedia.org/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [Alt+Shift+r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>

			<div class="body">
									<ul>
						<li id="t-whatlinkshere"><a href="https://en.wikipedia.org/wiki/Special:WhatLinksHere/Automatic_summarization" title="List of all English Wikipedia pages containing links to this page [Alt+Shift+j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Automatic_summarization" rel="nofollow" title="Recent changes in pages linked from this page [Alt+Shift+k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [Alt+Shift+u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="https://en.wikipedia.org/wiki/Special:SpecialPages" title="A list of all special pages [Alt+Shift+q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;oldid=774545866" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Q1394144" title="Link to connected data repository item [Alt+Shift+g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&amp;page=Automatic_summarization&amp;id=774545866" title="Information on how to cite this page">Cite this page</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>

			<div class="body">
									<ul>
						<li id="coll-create_a_book"><a href="https://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Automatic+summarization">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="https://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=render_article&amp;arttitle=Automatic+summarization&amp;returnto=Automatic+summarization&amp;oldid=774545866&amp;writer=rdf2latex">Download as PDF</a></li><li id="t-print"><a href="https://en.wikipedia.org/w/index.php?title=Automatic_summarization&amp;printable=yes" title="Printable version of this page [Alt+Shift+p]" accesskey="p">Printable version</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label"><span class="uls-settings-trigger" title="Language settings" tabindex="0" role="button" aria-haspopup="true"></span>
			<h3 id="p-lang-label">Languages</h3>

			<div class="body">
									<ul>
						<li class="interlanguage-link interwiki-ar"><a href="https://ar.wikipedia.org/wiki/%D8%A7%D9%84%D8%AA%D9%84%D8%AE%D9%8A%D8%B5_%D8%A7%D9%84%D8%AA%D9%84%D9%82%D8%A7%D8%A6%D9%8A" title="   Arabic" hreflang="ar" class="interlanguage-link-target" lang="ar"></a></li><li class="interlanguage-link interwiki-de"><a href="https://de.wikipedia.org/wiki/Text-Extraction" title="Text-Extraction  German" hreflang="de" class="interlanguage-link-target" lang="de">Deutsch</a></li><li class="interlanguage-link interwiki-eo"><a href="https://eo.wikipedia.org/wiki/A%C5%ADtomata_noticado" title="Atomata noticado  Esperanto" hreflang="eo" class="interlanguage-link-target" lang="eo">Esperanto</a></li><li class="interlanguage-link interwiki-eu"><a href="https://eu.wikipedia.org/wiki/Laburpengintza_automatikoa" title="Laburpengintza automatikoa  Basque" hreflang="eu" class="interlanguage-link-target" lang="eu">Euskara</a></li><li class="interlanguage-link interwiki-fa"><a href="https://fa.wikipedia.org/wiki/%D8%AE%D9%84%D8%A7%D8%B5%D9%87%E2%80%8C%D8%B3%D8%A7%D8%B2%DB%8C_%D8%AE%D9%88%D8%AF%DA%A9%D8%A7%D8%B1" title="   Persian" hreflang="fa" class="interlanguage-link-target" lang="fa"></a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/R%C3%A9sum%C3%A9_automatique_de_texte" title="Rsum automatique de texte  French" hreflang="fr" class="interlanguage-link-target" lang="fr">Franais</a></li><li class="interlanguage-link interwiki-ja"><a href="https://ja.wikipedia.org/wiki/%E8%87%AA%E5%8B%95%E8%A6%81%E7%B4%84" title="  Japanese" hreflang="ja" class="interlanguage-link-target" lang="ja"></a></li><li class="interlanguage-link interwiki-nn"><a href="https://nn.wikipedia.org/wiki/Automatisk_samandrag" title="Automatisk samandrag  Norwegian Nynorsk" hreflang="nn" class="interlanguage-link-target" lang="nn">Norsk nynorsk</a></li><li class="interlanguage-link interwiki-ro"><a href="https://ro.wikipedia.org/wiki/Sumarizare_automat%C4%83" title="Sumarizare automat  Romanian" hreflang="ro" class="interlanguage-link-target" lang="ro">Romn</a></li><li class="interlanguage-link interwiki-fi"><a href="https://fi.wikipedia.org/wiki/Tekstin_automaattinen_tiivist%C3%A4minen" title="Tekstin automaattinen tiivistminen  Finnish" hreflang="fi" class="interlanguage-link-target" lang="fi">Suomi</a></li><li class="interlanguage-link interwiki-sv"><a href="https://sv.wikipedia.org/wiki/Textsammanfattning" title="Textsammanfattning  Swedish" hreflang="sv" class="interlanguage-link-target" lang="sv">Svenska</a></li><li class="interlanguage-link interwiki-th"><a href="https://th.wikipedia.org/wiki/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%AA%E0%B8%A3%E0%B8%B8%E0%B8%9B%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1%E0%B8%AD%E0%B8%B1%E0%B8%95%E0%B9%82%E0%B8%99%E0%B8%A1%E0%B8%B1%E0%B8%95%E0%B8%B4" title="  Thai" hreflang="th" class="interlanguage-link-target" lang="th"></a></li><li class="interlanguage-link interwiki-uk"><a href="https://uk.wikipedia.org/wiki/%D0%90%D0%B2%D1%82%D0%BE%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%BD%D0%B5_%D1%80%D0%B5%D1%84%D0%B5%D1%80%D1%83%D0%B2%D0%B0%D0%BD%D0%BD%D1%8F" title="   Ukrainian" hreflang="uk" class="interlanguage-link-target" lang="uk"></a></li>					</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Q1394144#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
		<div id="footer" role="contentinfo">
							<ul id="footer-info">
											<li id="footer-info-lastmod"> This page was last modified on 9 April 2017, at 05:39.</li>
											<li id="footer-info-copyright">Text is available under the <a rel="license" href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="https://wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="https://wikimediafoundation.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia is a registered trademark of the <a href="https://www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
									</ul>
							<ul id="footer-places">
											<li id="footer-places-privacy"><a href="https://wikimediafoundation.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
											<li id="footer-places-about"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
											<li id="footer-places-disclaimer"><a href="https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
											<li id="footer-places-contact"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
											<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
											<li id="footer-places-cookiestatement"><a href="https://wikimediafoundation.org/wiki/Cookie_statement">Cookie statement</a></li>
											<li id="footer-places-mobileview"><a href="https://en.m.wikipedia.org/w/index.php?title=Automatic_summarization&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
									</ul>
										<ul id="footer-icons" class="noprint">
											<li id="footer-copyrightico">
							<a href="https://wikimediafoundation.org/"><img src="Automatic%20summarization%20-%20Wikipedia_files/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" alt="Wikimedia Foundation" width="88" height="31"></a>						</li>
											<li id="footer-poweredbyico">
							<a href="https://www.mediawiki.org/"><img src="Automatic%20summarization%20-%20Wikipedia_files/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"></a>						</li>
									</ul>
						<div style="clear:both"></div>
		</div>
		<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.320","walltime":"0.381","ppvisitednodes":{"value":1219,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":50476,"limit":2097152},"templateargumentsize":{"value":5908,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":3,"limit":500},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  313.576      1 -total"," 41.03%  128.661      1 Template:Reflist"," 34.09%  106.904      4 Template:Ambox"," 26.52%   83.154      1 Template:Multiple_issues"," 23.13%   72.527      5 Template:Cite_web"," 20.13%   63.124     15 Template:Cite_book","  9.81%   30.765      1 Template:More_footnotes","  3.72%   11.669      1 Template:Main_article","  3.39%   10.644      1 Template:Expand_section","  3.24%   10.163      1 Template:DOI"]},"scribunto":{"limitreport-timeusage":{"value":"0.153","limit":"10.000"},"limitreport-memusage":{"value":4510961,"limit":52428800}},"cachereport":{"origin":"mw1197","timestamp":"20170409053941","ttl":2592000,"transientcontent":false}}});});</script><script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":74,"wgHostname":"mw1216"});});</script>
	

<div style="display: none; font-size: 13px;" class="suggestions"><div class="suggestions-results"></div><div class="suggestions-special"></div></div><a accesskey="v" href="https://en.wikipedia.org/wiki/Automatic_summarization?action=edit&amp;oldid=774545866" style="display: none;"></a></body></html>